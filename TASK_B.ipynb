{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_token\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:07:44.185748Z","iopub.execute_input":"2025-06-10T16:07:44.186076Z","iopub.status.idle":"2025-06-10T16:07:44.286438Z","shell.execute_reply.started":"2025-06-10T16:07:44.186053Z","shell.execute_reply":"2025-06-10T16:07:44.285876Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -q accelerate\n!pip install -q transformers\n!pip install -q peft\n!pip install -q bitsandbytes\n!pip install -q trl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:07:44.287470Z","iopub.execute_input":"2025-06-10T16:07:44.287767Z","iopub.status.idle":"2025-06-10T16:08:13.965729Z","shell.execute_reply.started":"2025-06-10T16:07:44.287737Z","shell.execute_reply":"2025-06-10T16:08:13.964787Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.8/512.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import transformers, accelerate, bitsandbytes, trl\nprint(\"transformers version:\", transformers.__version__)\nprint(\"accelerate version:\", accelerate.__version__)\nprint(\"bitsandbytes version:\", bitsandbytes.__version__)\nprint(\"trl version:\", trl.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:08:13.967724Z","iopub.execute_input":"2025-06-10T16:08:13.968025Z","iopub.status.idle":"2025-06-10T16:08:19.216463Z","shell.execute_reply.started":"2025-06-10T16:08:13.968000Z","shell.execute_reply":"2025-06-10T16:08:19.215781Z"}},"outputs":[{"name":"stdout","text":"transformers version: 4.52.4\naccelerate version: 1.2.1\nbitsandbytes version: 0.46.0\ntrl version: 0.18.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n!pip install --no-deps cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n!pip install --no-deps unsloth\n!pip install transformers tf-keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:08:19.217698Z","iopub.execute_input":"2025-06-10T16:08:19.218122Z","iopub.status.idle":"2025-06-10T16:08:37.486401Z","shell.execute_reply.started":"2025-06-10T16:08:19.218096Z","shell.execute_reply":"2025-06-10T16:08:37.485574Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.46.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting xformers==0.0.29\n  Downloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.18.1)\nCollecting triton\n  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nDownloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, xformers\nSuccessfully installed triton-3.3.1 xformers-0.0.29\nCollecting cut_cross_entropy\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting unsloth_zoo\n  Downloading unsloth_zoo-2025.6.1-py3-none-any.whl.metadata (8.1 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading unsloth_zoo-2025.6.1-py3-none-any.whl (147 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\nSuccessfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.6.1\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.32.5)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (0.1.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (1.1.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nCollecting unsloth\n  Downloading unsloth-2025.6.2-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth-2025.6.2-py3-none-any.whl (276 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.0/277.0 kB\u001b[0m \u001b[31m620.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth\nSuccessfully installed unsloth-2025.6.2\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.52.4)\nRequirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.32.5)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: tensorflow<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.17.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.37.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.13.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.1.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Install PyTorch, torchvision, torchao nightlies\n!pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu126 # full options are cpu/cu118/cu121/cu124/cu126\n!pip install --pre torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu --no-cache-dir ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:08:37.487487Z","iopub.execute_input":"2025-06-10T16:08:37.487845Z","iopub.status.idle":"2025-06-10T16:11:35.758452Z","shell.execute_reply.started":"2025-06-10T16:08:37.487817Z","shell.execute_reply":"2025-06-10T16:11:35.757573Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/nightly/cu126\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.8.0.dev20250610%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torchvision-0.23.0.dev20250610%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\nCollecting torchao\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torchao-0.12.0.dev20250610%2Bcu126-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.26.5 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvshmem-cu12==3.2.5 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nvshmem_cu12-3.2.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (89 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting pytorch-triton==3.3.1+gitc8757738 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.3.1%2Bgitc8757738-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-triton==3.3.1+gitc8757738->torch) (75.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/nightly/cu126/torch-2.8.0.dev20250610%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl (820.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m820.8/820.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (318.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nvshmem_cu12-3.2.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (90.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.3.1%2Bgitc8757738-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/torchvision-0.23.0.dev20250610%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl (7.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/torchao-0.12.0.dev20250610%2Bcu126-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchao, nvidia-cusparselt-cu12, sympy, pytorch-triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nccl-cu12, nvidia-cufile-cu12, nvidia-cudnn-cu12, nvidia-cuda-nvrtc-cu12, torch, torchvision\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.6.2 requires tyro, which is not installed.\nunsloth-zoo 2025.6.1 requires msgspec, which is not installed.\nunsloth-zoo 2025.6.1 requires tyro, which is not installed.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\nunsloth 2025.6.2 requires datasets>=3.4.1, but you have datasets 3.3.1 which is incompatible.\nunsloth 2025.6.2 requires torch<=2.7.0,>=2.4.0, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\nunsloth-zoo 2025.6.1 requires datasets>=3.4.1, but you have datasets 3.3.1 which is incompatible.\nunsloth-zoo 2025.6.1 requires torch<=2.7.0, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\nxformers 0.0.29 requires torch==2.5.1, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufile-cu12-1.11.1.6 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.26.5 nvidia-nvshmem-cu12-3.2.5 nvidia-nvtx-cu12-12.6.77 pytorch-triton-3.3.1+gitc8757738 sympy-1.13.3 torch-2.8.0.dev20250610+cu126 torchao-0.12.0.dev20250610+cu126 torchvision-0.23.0.dev20250610+cu126\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\nRequirement already satisfied: torchtune in /usr/local/lib/python3.10/dist-packages (0.5.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from torchtune) (3.3.1)\nRequirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.32.5)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.4.5)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.3.9)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.2.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.9.0)\nRequirement already satisfied: blobfile>=2 in /usr/local/lib/python3.10/dist-packages (from torchtune) (3.0.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtune) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtune) (4.67.1)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from torchtune) (2.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtune) (5.9.5)\nRequirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from torchtune) (11.0.0)\nRequirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (3.21.0)\nRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (2.3.0)\nRequirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (5.3.0)\nRequirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (3.11.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (2.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (1.1.3)\nRequirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->torchtune) (4.9.3)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->torchtune) (2024.11.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtune) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtune) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torch.distributed.fsdp import fully_shard\n\nprint(\"Torch version:\", torch.__version__) \nprint(\"Successfully imported fully_shard!\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:53.305363Z","iopub.execute_input":"2025-06-10T16:11:53.305945Z","iopub.status.idle":"2025-06-10T16:11:53.312487Z","shell.execute_reply.started":"2025-06-10T16:11:53.305915Z","shell.execute_reply":"2025-06-10T16:11:53.311241Z"}},"outputs":[{"name":"stdout","text":"Torch version: 2.8.0.dev20250610+cu126\nSuccessfully imported fully_shard!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# !pip install --pre --upgrade \\\n#    torch torchvision torchaudio \\\n#    --extra-index-url https://download.pytorch.org/whl/nightly/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:35.795900Z","iopub.status.idle":"2025-06-10T16:11:35.796136Z","shell.execute_reply":"2025-06-10T16:11:35.796041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy==1.23.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:12:46.544209Z","iopub.execute_input":"2025-06-10T16:12:46.544495Z","iopub.status.idle":"2025-06-10T16:12:54.008757Z","shell.execute_reply.started":"2025-06-10T16:12:46.544473Z","shell.execute_reply":"2025-06-10T16:12:54.007723Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.6.2 requires tyro, which is not installed.\nunsloth-zoo 2025.6.1 requires msgspec, which is not installed.\nunsloth-zoo 2025.6.1 requires tyro, which is not installed.\nalbucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nalbumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nchex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\njax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\npymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nunsloth 2025.6.2 requires datasets>=3.4.1, but you have datasets 3.3.1 which is incompatible.\nunsloth 2025.6.2 requires torch<=2.7.0,>=2.4.0, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\nunsloth-zoo 2025.6.1 requires datasets>=3.4.1, but you have datasets 3.3.1 which is incompatible.\nunsloth-zoo 2025.6.1 requires torch<=2.7.0, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nxarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nxformers 0.0.29 requires torch==2.5.1, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.distributed.fsdp as fsdp\nimport inspect\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"FSDP module location:\", inspect.getfile(fsdp))\nprint(\"\\nAttributes in torch.distributed.fsdp:\")\nfor name, member in inspect.getmembers(fsdp):\n    print(f\"{name}: {type(member)}\")\n\n# Check if fully_shard exists and print its docstring\nif hasattr(fsdp, \"fully_shard\"):\n    print(\"\\nFunction 'fully_shard' is available.\")\n    print(\"Docstring for fully_shard:\\n\", fsdp.fully_shard.__doc__) \nelse:\n    print(\"\\nFunction 'fully_shard' is NOT available in this build.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:12:54.010092Z","iopub.execute_input":"2025-06-10T16:12:54.010365Z","iopub.status.idle":"2025-06-10T16:12:54.026377Z","shell.execute_reply.started":"2025-06-10T16:12:54.010332Z","shell.execute_reply":"2025-06-10T16:12:54.023714Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.8.0.dev20250610+cu126\nFSDP module location: /usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/__init__.py\n\nAttributes in torch.distributed.fsdp:\nBackwardPrefetch: <class 'enum.EnumMeta'>\nCPUOffload: <class 'type'>\nCPUOffloadPolicy: <class 'type'>\nFSDPModule: <class 'type'>\nFlatParameter: <class 'torch.distributed.fsdp._flat_param._FlatParameterMeta'>\nFullOptimStateDictConfig: <class 'type'>\nFullStateDictConfig: <class 'type'>\nFullyShardedDataParallel: <class 'type'>\nLocalOptimStateDictConfig: <class 'type'>\nLocalStateDictConfig: <class 'type'>\nMixedPrecision: <class 'type'>\nMixedPrecisionPolicy: <class 'type'>\nOffloadPolicy: <class 'type'>\nOptimStateDictConfig: <class 'type'>\nOptimStateKeyType: <class 'enum.EnumMeta'>\nShardedOptimStateDictConfig: <class 'type'>\nShardedStateDictConfig: <class 'type'>\nShardingStrategy: <class 'enum.EnumMeta'>\nStateDictConfig: <class 'type'>\nStateDictSettings: <class 'type'>\nStateDictType: <class 'enum.EnumMeta'>\nUnshardHandle: <class 'type'>\n__all__: <class 'list'>\n__builtins__: <class 'dict'>\n__cached__: <class 'str'>\n__doc__: <class 'NoneType'>\n__file__: <class 'str'>\n__loader__: <class '_frozen_importlib_external.SourceFileLoader'>\n__name__: <class 'str'>\n__package__: <class 'str'>\n__path__: <class 'list'>\n__spec__: <class '_frozen_importlib.ModuleSpec'>\n_common_utils: <class 'module'>\n_debug_utils: <class 'module'>\n_dynamo_utils: <class 'module'>\n_exec_order_utils: <class 'module'>\n_flat_param: <class 'module'>\n_fsdp_extensions: <class 'module'>\n_fully_shard: <class 'module'>\n_init_utils: <class 'module'>\n_limiter_utils: <class 'module'>\n_optim_utils: <class 'module'>\n_runtime_utils: <class 'module'>\n_shard_utils: <class 'module'>\n_state_dict_utils: <class 'module'>\n_traversal_utils: <class 'module'>\n_unshard_param_utils: <class 'module'>\n_wrap_utils: <class 'module'>\napi: <class 'module'>\nfully_shard: <class 'function'>\nfully_sharded_data_parallel: <class 'module'>\nregister_fsdp_forward_method: <class 'function'>\nwrap: <class 'module'>\n\nFunction 'fully_shard' is available.\nDocstring for fully_shard:\n \n    Apply fully sharded data parallelism (FSDP) to ``module``, where FSDP\n    shards module parameters, gradients, and optimizer states across data\n    parallel workers to save memory at the cost of communication.\n\n    At initialization, FSDP shards the module's parameters across the data\n    parallel workers given by ``mesh``. Before forward, FSDP all-gathers the\n    sharded parameters across the data-parallel workers to get the unsharded\n    parameters for forward computation. If ``reshard_after_forward`` is\n    ``True``, then FSDP frees the unsharded parameters after forward and\n    re-all-gathers them in backward before gradient computation. After gradient\n    computation, FSDP frees the unsharded parameters and reduce-scatters the\n    unsharded gradients across data-parallel workers.\n\n    This implementation represents the sharded parameters as :class:`DTensor` s\n    sharded on dim-0, while the unsharded parameters will be like the original\n    parameters on ``module`` (e.g. :class:`torch.Tensor` if originally\n    :class:`torch.Tensor`). A module\n    `forward pre-hook <https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook>`_\n    on ``module`` all-gathers the parameters, and a module\n    `forward hook <https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook>`_\n    on ``module`` frees them (if needed). Similar backward hooks all-gather\n    parameters and later free parameters and reduce-scatter gradients.\n\n    Since grouping multiple tensors together for one collective is critical for\n    communication efficiency, this implementation makes this grouping first\n    class. Calling :meth:`fully_shard` on ``module`` constructs one group that\n    includes the parameters in ``module.parameters()`` except those already\n    assigned to a group from an earlier call on a submodule. This means that\n    :meth:`fully_shard` should be called bottom-up on your model. Each group's\n    parameters are all-gathered in one collective, and its gradients are\n    reduce-scattered in one collective. Partitioning the model into multiple\n    groups (\"layer by layer\") allows for peak memory savings and communication/computation\n    overlap. Users generally should *not* call :meth:`fully_shard` only on the\n    topmost root module.\n\n    Args:\n        module (Union[nn.Module, List[nn.Module]): The module or modules to\n            shard with FSDP and group together for communication.\n        mesh (Optional[DeviceMesh]): This data parallel mesh defines the\n            sharding and device. If 1D, then parameters are fully sharded\n            across the 1D mesh (FSDP) with ``(Shard(0),)`` placement. If 2D,\n            then parameters are sharded across the 1st dim and replicated\n            across the 0th dim (HSDP) with ``(Replicate(), Shard(0))``\n            placement. The mesh's device type gives the device type used for\n            communication; if a CUDA or CUDA-like device type, then we use the\n            current device.\n        reshard_after_forward (Optional[Union[bool, int]]): This controls the parameter\n            behavior after forward and can trade off memory and communication:\n\n            - If ``True``, then this reshards parameters after forward and\n              re-all-gathers in backward.\n            - If ``False``, then this keeps the unsharded parameters in memory\n              after forward and avoids the all-gather in backward. For best performance,\n              we usually set ``False`` for the root module, because the root module\n              is typically required immediately when the backward pass begins.\n            - If ``None``, it is set to ``True`` for non-root modules and ``False``\n              for root modules.\n            - If an ``int``, then this represents the world size to reshard to\n              after forward. It should be a non-trivial divisor of the ``mesh``\n              shard dim size (i.e. excluding 1 and the dim size itself). A\n              choice may be the intra-node size (e.g. ``torch.cuda.device_count()``).\n              This allows the all-gather in backward to be over a smaller world\n              size at the cost of higher memory usage than setting to ``True``.\n            - After forward, the parameters registered to the module depend on\n              to this: The registered parameters are the sharded parameters if\n              ``True``; unsharded parameters if ``False``; and the paramters\n              resharded to the smaller mesh otherwise. To modify the parameters\n              between forward and backward, the registered parameters must be\n              the sharded parameters. For ``False`` or an ``int``, this can be\n              done by manually resharding via :meth:`reshard`.\n        shard_placement_fn (Optional[Callable[[nn.Parameter], Optional[Shard]]]):\n            This callable can be used to override the sharding placement for a\n            parameter to shard a parameter on a dimension other than dim-0. If\n            this callable returns a :class:`Shard` placement (not ``None``),\n            then FSDP will shard according to that placement (e.g. ``Shard(1)``).\n            If sharding on a nonzero dim, we currently require even sharding,\n            i.e. the tensor dim size on that dim must be divisible by the FSDP\n            shard mesh size.\n        mp_policy (MixedPrecisionPolicy): This controls the mixed precision\n            policy, which offers parameter/reduction mixed precision for this\n            module. See :class:`MixedPrecisionPolicy` for details.\n        offload_policy (OffloadPolicy): This controls the offloading policy,\n            which offers parameter/gradient/optimizer state offloading. See\n            :class:`OffloadPolicy` and its subclasses for details.\n        ignored_params: Optional(Set[nn.Parameter]): The set of parameters to be\n            ignored by FSDP. They will not be sharded, nor moved to the device\n            during init, nor have their gradients reduced in backward.\n\n    Returns:\n        FSDPModule: The module with FSDP applied (in-place).\n    \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install requests hydra-core --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:33:24.708561Z","iopub.execute_input":"2025-06-10T16:33:24.708943Z","iopub.status.idle":"2025-06-10T16:33:28.843242Z","shell.execute_reply.started":"2025-06-10T16:33:24.708911Z","shell.execute_reply":"2025-06-10T16:33:28.842365Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nCollecting requests\n  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (1.3.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.1.31)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (24.2)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\nDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: requests\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Uninstalling requests-2.32.3:\n      Successfully uninstalled requests-2.32.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.6.2 requires tyro, which is not installed.\nunsloth-zoo 2025.6.1 requires msgspec, which is not installed.\nunsloth-zoo 2025.6.1 requires tyro, which is not installed.\nbigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nunsloth 2025.6.2 requires datasets>=3.4.1, but you have datasets 3.3.1 which is incompatible.\nunsloth 2025.6.2 requires torch<=2.7.0,>=2.4.0, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\nunsloth-zoo 2025.6.1 requires datasets>=3.4.1, but you have datasets 3.3.1 which is incompatible.\nunsloth-zoo 2025.6.1 requires torch<=2.7.0, but you have torch 2.8.0.dev20250610+cu126 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed requests-2.32.4\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile fsdp2_qlora_final_solution.py\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import fully_shard, MixedPrecision\nfrom torch.nn.utils import clip_grad_norm_ # Correct import for gradient clipping\nfrom torch.distributed._tensor import DeviceMesh\nfrom torch.utils.data import DataLoader\nfrom torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nimport time\nimport triton\nimport triton.language as tl\nimport math\nimport gc\n\n# Environment setup\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\n# Your Custom Triton Kernel\n@triton.jit\ndef _your_dequantize_nf4_kernel(\n    w_ptr, abs_idx_ptr, offset_ptr, abs2_scales_ptr, code2_ptr, nf4_code_ptr, output_ptr,\n    TOTAL_ELEMENTS_IN_OUTPUT_TENSOR: tl.constexpr, BLOCK_SIZE_BYTES_PER_CHUNK: tl.constexpr,\n    BLOCK_SIZE_ELEMENTS_PER_CHUNK: tl.constexpr, NUM_GROUPS_PER_CHUNK: tl.constexpr,\n    ELEMENTS_PER_GROUP_CONST: tl.constexpr, LOG2_L2_BLOCK_SIZE_CONST_KERNEL: tl.constexpr,\n    gsize_num_chunks: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= gsize_num_chunks: return\n    log2_l2_block_size = LOG2_L2_BLOCK_SIZE_CONST_KERNEL\n    chunk_element_start_offset = pid * BLOCK_SIZE_ELEMENTS_PER_CHUNK\n    group_arange_local = tl.arange(0, NUM_GROUPS_PER_CHUNK)\n    absmax_group_indices_potential = (chunk_element_start_offset // ELEMENTS_PER_GROUP_CONST) + group_arange_local\n    group_mask = (absmax_group_indices_potential * ELEMENTS_PER_GROUP_CONST) < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n    quantized_absmax_indices = tl.load(abs_idx_ptr + absmax_group_indices_potential, mask=group_mask, other=0, eviction_policy=\"evict_first\")\n    dequantized_l1_scales = tl.load(code2_ptr + quantized_absmax_indices.to(tl.int32), mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n    absmax_l2_group_indices_potential = absmax_group_indices_potential >> log2_l2_block_size\n    l2_scales = tl.load(abs2_scales_ptr + absmax_l2_group_indices_potential, mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n    offset_val = tl.load(offset_ptr + 0)\n    intermediate_product_scales = l2_scales * dequantized_l1_scales\n    final_group_scales_masked = intermediate_product_scales + offset_val\n    element_arange_pid_local = tl.arange(0, BLOCK_SIZE_ELEMENTS_PER_CHUNK)\n    global_element_indices = chunk_element_start_offset + element_arange_pid_local\n    element_op_mask = global_element_indices < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n    byte_index_global_for_element = global_element_indices // 2\n    is_low_nibble_flag = (global_element_indices % 2) != 0\n    packed_byte_for_element = tl.load(w_ptr + byte_index_global_for_element, mask=element_op_mask, other=0)\n    nibble_shift = tl.where(is_low_nibble_flag, 0, 4)\n    quantized_idx_for_element = (packed_byte_for_element >> nibble_shift) & 0x0F\n    dequant_val_for_element = tl.load(nf4_code_ptr + quantized_idx_for_element.to(tl.int32), mask=element_op_mask, other=0.0)\n    scales_reshaped_for_broadcast = tl.reshape(final_group_scales_masked, (NUM_GROUPS_PER_CHUNK, 1))\n    scales_broadcasted_to_elements = tl.broadcast_to(scales_reshaped_for_broadcast, (NUM_GROUPS_PER_CHUNK, ELEMENTS_PER_GROUP_CONST))\n    element_scales_vector = tl.reshape(scales_broadcasted_to_elements, (BLOCK_SIZE_ELEMENTS_PER_CHUNK,))\n    final_scale_for_element = tl.where(element_op_mask, element_scales_vector, 0.0)\n    scaled_element_output = dequant_val_for_element * final_scale_for_element\n    tl.store(output_ptr + global_element_indices, scaled_element_output, mask=element_op_mask)\n    return\n\ndef _your_dequantize_nf4(\n    weight_data_actual, quant_state_actual, CHUNK_SIZE_ELEMENTS_Br: int,\n    num_warps_to_use: int, num_stages_to_use: int\n):\n    device = weight_data_actual.device; output_shape = quant_state_actual.shape\n    total_elements_in_output = output_shape.numel()\n    if total_elements_in_output == 0: return torch.empty(output_shape, device=device, dtype=quant_state_actual.dtype)\n    props = torch.cuda.get_device_properties(device); is_t4 = (props.major == 7 and props.minor == 5)\n    final_out_dtype = quant_state_actual.dtype\n    if is_t4 and final_out_dtype == torch.bfloat16: final_out_dtype = torch.float16\n    output_tensor = torch.empty(output_shape, dtype=final_out_dtype, device=device, requires_grad=False)\n    current_br = CHUNK_SIZE_ELEMENTS_Br; ELEMENTS_PER_GROUP_CONST = quant_state_actual.blocksize\n    L2_BLOCK_SIZE_CONST_PYTHON = quant_state_actual.state2.blocksize\n    if not (L2_BLOCK_SIZE_CONST_PYTHON > 0 and (L2_BLOCK_SIZE_CONST_PYTHON & (L2_BLOCK_SIZE_CONST_PYTHON - 1) == 0)):\n        raise ValueError(f\"L2_BLOCK_SIZE_CONST_PYTHON must be a power of 2 and > 0. Got {L2_BLOCK_SIZE_CONST_PYTHON}\")\n    if total_elements_in_output < current_br:\n        current_br = triton.cdiv(total_elements_in_output, ELEMENTS_PER_GROUP_CONST) * ELEMENTS_PER_GROUP_CONST \\\n            if total_elements_in_output > ELEMENTS_PER_GROUP_CONST else ELEMENTS_PER_GROUP_CONST\n    if current_br == 0 and total_elements_in_output > 0: current_br = ELEMENTS_PER_GROUP_CONST\n    elif current_br < ELEMENTS_PER_GROUP_CONST and total_elements_in_output > 0: current_br = ELEMENTS_PER_GROUP_CONST\n    CHUNK_SIZE_BYTES_Br_half = current_br // 2\n    NUM_GROUPS_PER_CHUNK_Br_div_64 = current_br // ELEMENTS_PER_GROUP_CONST\n    if NUM_GROUPS_PER_CHUNK_Br_div_64 == 0 and total_elements_in_output > 0:\n        current_br = ELEMENTS_PER_GROUP_CONST; NUM_GROUPS_PER_CHUNK_Br_div_64 = 1\n    LOG2_L2_BLOCK_SIZE_CONST_PYTHON = int(math.log2(L2_BLOCK_SIZE_CONST_PYTHON))\n    gsize_num_chunks_val = triton.cdiv(total_elements_in_output, current_br)\n    if gsize_num_chunks_val == 0 and total_elements_in_output > 0: gsize_num_chunks_val = 1\n    grid = (gsize_num_chunks_val,)\n    const_args_for_kernel = {'TOTAL_ELEMENTS_IN_OUTPUT_TENSOR': total_elements_in_output,'BLOCK_SIZE_BYTES_PER_CHUNK': CHUNK_SIZE_BYTES_Br_half,'BLOCK_SIZE_ELEMENTS_PER_CHUNK': current_br, 'NUM_GROUPS_PER_CHUNK': NUM_GROUPS_PER_CHUNK_Br_div_64,'ELEMENTS_PER_GROUP_CONST': ELEMENTS_PER_GROUP_CONST,'LOG2_L2_BLOCK_SIZE_CONST_KERNEL': LOG2_L2_BLOCK_SIZE_CONST_PYTHON,'gsize_num_chunks': gsize_num_chunks_val}\n    _your_dequantize_nf4_kernel[grid](\n        weight_data_actual, quant_state_actual.absmax, quant_state_actual.offset,\n        quant_state_actual.state2.absmax, quant_state_actual.state2.code, quant_state_actual.code,\n        output_tensor, **const_args_for_kernel, num_warps=num_warps_to_use, num_stages=num_stages_to_use\n    )\n    return output_tensor\n\ndef your_dequantize_nf4(weight_param):\n    OPTIMIZED_BR=4096; OPTIMIZED_WARPS=4; OPTIMIZED_STAGES=2\n    return _your_dequantize_nf4(\n        weight_param.weight.data, weight_param.weight.quant_state,\n        OPTIMIZED_BR, OPTIMIZED_WARPS, OPTIMIZED_STAGES)\n\ndef initialize_distributed():\n    rank=int(os.environ.get(\"RANK\",0)); local_rank=int(os.environ.get(\"LOCAL_RANK\",0))\n    world_size=int(os.environ.get(\"WORLD_SIZE\",1))\n    if not dist.is_initialized(): dist.init_process_group(\"nccl\")\n    torch.cuda.set_device(local_rank)\n    print(f\"[Rank {rank}] Initialized with local_rank {local_rank} out of {world_size} processes.\")\n    return rank, local_rank, world_size\n\n@torch.compile(fullgraph=True, options={\"triton.cudagraphs\": False})\ndef compiled_dequantize_forward(x, weight): return x @ weight.t()\n\ndef patch_bnb_for_custom_dequant():\n    from bitsandbytes.nn.modules import Linear4bit\n    print(\"Patching Linear4bit with custom Triton dequantization kernel (CUDAGraphs disabled).\")\n    Linear4bit.forward = lambda self, x: compiled_dequantize_forward(x, your_dequantize_nf4(self))\n\nclass LoRAWrapper(nn.Module):\n    def __init__(self, model, rank, world_size):\n        super().__init__()\n        self.model = model\n        \n        self.lora_params_module = nn.Module()\n        self.lora_params_module.lora_params = nn.ParameterList([p for p in model.parameters() if p.requires_grad])\n\n        if rank == 0:\n            print(f\"Extracted {len(self.lora_params_module.lora_params)} trainable parameters for FSDP wrapping.\")\n\n        if len(self.lora_params_module.lora_params) > 0:\n            device_mesh = DeviceMesh(\"cuda\", torch.arange(world_size))\n            mp_policy = MixedPrecision(param_dtype=torch.float16, reduce_dtype=torch.float16)\n            fully_shard(self.lora_params_module, mesh=device_mesh, mp_policy=mp_policy)\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n    def parameters(self, recurse: bool = True):\n        return self.lora_params_module.parameters(recurse)\n\n    def named_parameters(self, prefix: str = '', recurse: bool = True):\n        return self.lora_params_module.named_parameters(prefix, recurse)\n        \n    def __getattr__(self, name: str):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.model, name)\n\ndef main():\n    rank, local_rank, world_size = initialize_distributed()\n    if rank == 0: print(\"Patching BitsAndBytes...\")\n    patch_bnb_for_custom_dequant()\n    torch.set_default_dtype(torch.float16)\n    \n    model_name = \"unsloth/Llama-3.1-8B-Instruct\"\n    max_seq_length = 2048\n    if rank == 0: print(f\"Loading 16-bit model '{model_name}' and quantizing for QLoRA...\")\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16,\n        device_map={\"\": local_rank},\n        attn_implementation=\"sdpa\",\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token; tokenizer.padding_side = \"right\"\n    \n    lora_config = LoraConfig(\n        r=64, lora_alpha=128,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_dropout=0, bias=\"none\", task_type=TaskType.CAUSAL_LM,\n    )\n    peft_model = get_peft_model(model, lora_config)\n    peft_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n    \n    fsdp2_model = LoRAWrapper(peft_model, rank, world_size)\n    \n    url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n    dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")\n            \n    training_args = SFTConfig(\n        output_dir=\"./fsdp2_qlora_solution\", \n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        warmup_steps=5, max_steps=60, learning_rate=2e-4, fp16=True,\n        logging_steps=5,\n        max_seq_length=max_seq_length,\n        dataset_num_proc=2,\n        report_to=\"none\", seed=3407,\n    )\n    \n    temp_trainer = SFTTrainer(model=peft_model, args=training_args, train_dataset=dataset)\n    train_dataloader = temp_trainer.get_train_dataloader()\n\n    optimizer = torch.optim.AdamW(fsdp2_model.parameters(), lr=training_args.learning_rate)\n    # --- CRITICAL FIX: Use the FSDP-native ShardedGradScaler ---\n    scaler = ShardedGradScaler()\n    # --- END FIX ---\n\n    if rank == 0:\n        print(\"Starting training with MANUAL LOOP, FSDP2 on LoRA, QLoRA, custom kernel...\")\n        trainable_params = sum(p.numel() for p in fsdp2_model.parameters())\n        print(f\"Trainable LoRA parameters (FSDP2-sharded): {trainable_params:,}\")\n\n    fsdp2_model.train()\n    global_step = 0\n    for _ in range(10): \n        for batch in train_dataloader:\n            if global_step >= training_args.max_steps: break\n            \n            with torch.cuda.amp.autocast():\n                outputs = fsdp2_model(\n                    input_ids=batch['input_ids'].to(local_rank),\n                    attention_mask=batch['attention_mask'].to(local_rank),\n                    labels=batch['labels'].to(local_rank)\n                )\n                loss = outputs.loss\n            \n            loss = loss / training_args.gradient_accumulation_steps\n            scaler.scale(loss).backward()\n\n            if (global_step + 1) % training_args.gradient_accumulation_steps == 0:\n                # The ShardedGradScaler handles unscaling and inf checks correctly with FSDP.\n                # Gradient clipping should be applied to the parameters *after* unscaling.\n                scaler.unscale_(optimizer)\n                \n                # The total norm is calculated over all gradients of the passed-in parameters.\n                # For FSDP, this call is aware of the sharded nature of the parameters\n                # and will correctly compute the global norm.\n                total_norm = clip_grad_norm_(fsdp2_model.parameters(), max_norm=1.0)\n                \n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n\n            global_step += 1\n            if rank == 0 and global_step % training_args.logging_steps == 0:\n                # Only print grad norm when an optimizer step is taken\n                if (global_step + 1) % training_args.gradient_accumulation_steps == 0:\n                    print(f\"Step {global_step}/{training_args.max_steps}, Loss: {loss.item() * training_args.gradient_accumulation_steps:.4f}, Grad Norm: {total_norm.item():.4f}\")\n                else:\n                    print(f\"Step {global_step}/{training_args.max_steps}, Loss: {loss.item() * training_args.gradient_accumulation_steps:.4f}\")\n        if global_step >= training_args.max_steps: break\n\n    dist.barrier()\n    if rank == 0:\n        print(\"Training completed!\")\n        print(\"Attempting to save LoRA adapters...\")\n        try:\n            from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n            from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n            \n            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n            with FSDP.state_dict_type(fsdp2_model.lora_params_module, StateDictType.FULL_STATE_DICT, save_policy):\n                cpu_state = fsdp2_model.lora_params_module.state_dict()\n            \n            if rank == 0:\n                fsdp2_model.model.load_state_dict(cpu_state, strict=False)\n                save_dir = \"./final_model_adapters\"\n                fsdp2_model.model.save_pretrained(save_dir)\n                tokenizer.save_pretrained(save_dir)\n                print(f\"LoRA adapters saved to {save_dir}\")\n        except Exception as e:\n            print(f\"Error during model saving: {e}\")\n    \n    gc.collect(); torch.cuda.empty_cache()\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:36:43.596832Z","iopub.execute_input":"2025-06-10T20:36:43.597245Z","iopub.status.idle":"2025-06-10T20:36:43.605322Z","shell.execute_reply.started":"2025-06-10T20:36:43.597210Z","shell.execute_reply":"2025-06-10T20:36:43.604580Z"}},"outputs":[{"name":"stdout","text":"Overwriting fsdp2_qlora_final_solution.py\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"!accelerate launch --multi_gpu --num_processes 2 fsdp2_qlora_final_solution.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:36:43.789203Z","iopub.execute_input":"2025-06-10T20:36:43.789400Z","iopub.status.idle":"2025-06-10T20:38:25.576650Z","shell.execute_reply.started":"2025-06-10T20:36:43.789383Z","shell.execute_reply":"2025-06-10T20:38:25.575788Z"}},"outputs":[{"name":"stdout","text":"2025-06-10 20:36:55.030178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-10 20:36:55.052449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-10 20:36:55.059593: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-10 20:36:55.264959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-10 20:36:55.286967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-10 20:36:55.293637: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[Rank 0] Initialized with local_rank 0 out of 2 processes.\nPatching BitsAndBytes...\n[Rank 1] Initialized with local_rank 1 out of 2 processes.\nPatching Linear4bit with custom Triton dequantization kernel (CUDAGraphs disabled).\nLoading 16-bit model 'unsloth/Llama-3.1-8B-Instruct' and quantizing for QLoRA...\nPatching Linear4bit with custom Triton dequantization kernel (CUDAGraphs disabled).\nLoading checkpoint shards: 100%|██████████████████| 4/4 [00:23<00:00,  5.89s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [00:24<00:00,  6.02s/it]\nExtracted 448 trainable parameters for FSDP wrapping.\n/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:4770: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n[rank1]:[W610 20:37:28.337537466 ProcessGroupNCCL.cpp:4917] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:4770: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n[rank0]:[W610 20:37:29.112113776 ProcessGroupNCCL.cpp:4917] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\nStarting training with MANUAL LOOP, FSDP2 on LoRA, QLoRA, custom kernel...\nTrainable LoRA parameters (FSDP2-sharded): 167,772,160\n/kaggle/working/fsdp2_qlora_final_solution.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/kaggle/working/fsdp2_qlora_final_solution.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/kaggle/working/fsdp2_qlora_final_solution.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/fsdp2_qlora_final_solution.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/fsdp2_qlora_final_solution.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/fsdp2_qlora_final_solution.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nStep 5/60, Loss: 1.9951\nStep 10/60, Loss: 3.1939\nStep 15/60, Loss: 3.9753, Grad Norm: 0.0000\nStep 20/60, Loss: 3.2252\nStep 25/60, Loss: 2.4712\nStep 30/60, Loss: 3.0877\nStep 35/60, Loss: 2.2829\nStep 40/60, Loss: 1.9982\nStep 45/60, Loss: 2.2310\nStep 50/60, Loss: 1.9049\nStep 55/60, Loss: 2.1901, Grad Norm: 0.0000\nStep 60/60, Loss: 3.9063\nTraining completed!\nAttempting to save LoRA adapters...\n/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\nError during model saving: None\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}