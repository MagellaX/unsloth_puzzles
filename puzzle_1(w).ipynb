{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e161a016-8d1c-468c-84ea-4ab57ccdca76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e161a016-8d1c-468c-84ea-4ab57ccdca76",
        "outputId": "54d2aa61-d910-4cab-9bd0-28fa28014164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting xformers==0.0.29\n",
            "  Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.16.0-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, trl, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.4 trl-0.16.0 xformers-0.0.29\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\n",
            "Successfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.3.17\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, hf_transfer, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.3.17 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 hf_transfer-0.1.9 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.3.19\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install transformers tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "104f51e5-1fe9-430a-8d10-5bff70fe5fb6",
      "metadata": {
        "id": "104f51e5-1fe9-430a-8d10-5bff70fe5fb6"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=1e-1, rtol=1e-1)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d357bf28-e502-49b8-a6db-10564469c1ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d357bf28-e502-49b8-a6db-10564469c1ad",
        "outputId": "7296153f-7426-4c4d-f838-76e920697359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-6109ecb06959>:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth.kernels.utils import fast_dequantize\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        # (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        # (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4656412-4dde-45a0-b73c-e15407a3f161",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4656412-4dde-45a0-b73c-e15407a3f161",
        "outputId": "31952f2d-41a9-451a-f0aa-b016db888f73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1017403602600098"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f0f657f7-2c56-4ea5-9073-765a71b747f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f657f7-2c56-4ea5-9073-765a71b747f1",
        "outputId": "ebe2c47c-9c1e-43ab-e95d-0bedd1048733"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1417860984802246"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(w_ptr, w_out, abs_ptrs,\n",
        "                               offset_ptr,\n",
        "                               abs2_ptrs, code2,\n",
        "                               block_size2, gsize, code, blocks: tl.constexpr, Br: tl.constexpr):\n",
        "    \"\"\"\n",
        "    Optimized kernel for dequantizing NF4 weights\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    # Guard condition improves wave-front scheduling\n",
        "    if pid < gsize:\n",
        "        # Compute indices for absmax values with optimal coalescing\n",
        "        absmax_group = pid*blocks + tl.arange(0, blocks)\n",
        "\n",
        "        # Load absmax values with cache control\n",
        "        absmax = tl.load(abs_ptrs + absmax_group, eviction_policy=\"evict_first\")\n",
        "\n",
        "        # Use inline assembly for efficient log2 calculation\n",
        "        lz = tl.inline_asm_elementwise(\n",
        "            asm=\"\"\"\n",
        "            {\n",
        "                clz.b32 $0, $1;\n",
        "            }\n",
        "            \"\"\",\n",
        "            constraints=(\"=r,r\"),\n",
        "            args=[block_size2],\n",
        "            dtype=(tl.int32),\n",
        "            is_pure=True,\n",
        "            pack=1,\n",
        "        )\n",
        "\n",
        "        # Calculate second-level absmax indices using CLZ result\n",
        "        absmax_group2 = (absmax_group)>>(31-lz)\n",
        "\n",
        "        # Load scale factors with appropriate cache policies\n",
        "        real_absmax = tl.load(code2 + absmax, eviction_policy=\"evict_last\")\n",
        "        absmax2 = tl.load(abs2_ptrs + absmax_group2, eviction_policy=\"evict_last\")\n",
        "        offset = tl.load(offset_ptr, eviction_policy=\"evict_last\")\n",
        "\n",
        "        # Calculate final scale factors with fused multiply-add\n",
        "        final_absmax = absmax2 * real_absmax + offset\n",
        "\n",
        "        # Calculate weight offsets using 2D memory pattern\n",
        "        w_off = pid*(Br//2) + tl.arange(0, blocks)[:, None]*(Br//(2*blocks)) + tl.arange(0, Br//(2*blocks))[None, :]\n",
        "\n",
        "        # Load packed weights\n",
        "        w_packed = tl.load(w_ptr + w_off, eviction_policy=\"evict_first\")\n",
        "\n",
        "        # Interleave weights with themselves\n",
        "        w_packed2 = tl.interleave(w_packed, w_packed)\n",
        "\n",
        "        # Calculate shift amounts for each position\n",
        "        shift_sh = tl.arange(0, blocks)[:, None]*(Br//(blocks)) + tl.arange(0, Br//(blocks))[None, :]\n",
        "        shift = tl.where(shift_sh % 2 == 0, 4, 0)\n",
        "\n",
        "        # Extract 4-bit values using calculated shifts\n",
        "        shifted_w = (w_packed2 >> shift) & 0xF\n",
        "\n",
        "        # Load dequantized values from codebook\n",
        "        real_w = tl.load(shifted_w + code, eviction_policy=\"evict_last\")\n",
        "\n",
        "        # Apply scaling with broadcasting\n",
        "        scaled_w = (real_w * final_absmax[:, None])\n",
        "\n",
        "        # Calculate output offsets using tiled pattern\n",
        "        out_off = pid*Br + tl.arange(0, blocks)[:, None]*(Br//blocks) + tl.arange(0, Br//blocks)[None, :]\n",
        "\n",
        "        # Store results\n",
        "        tl.store(w_out + out_off, scaled_w, eviction_policy=\"evict_first\")\n",
        "    return\n",
        "\n",
        "def _your_dequantize_nf4(weight, quant_state):\n",
        "    \"\"\"\n",
        "    Internal implementation function\n",
        "    \"\"\"\n",
        "    # Get the current device\n",
        "    device = weight.device\n",
        "\n",
        "    # Extract metadata from quantization state\n",
        "    out_dtype = quant_state.dtype\n",
        "    code = quant_state.code\n",
        "    absmax = quant_state.absmax\n",
        "    real_shape = quant_state.shape\n",
        "    block_size = quant_state.blocksize\n",
        "    absmax2 = quant_state.state2.absmax\n",
        "    code2 = quant_state.state2.code\n",
        "    block_size2 = quant_state.state2.blocksize\n",
        "    offset = quant_state.offset\n",
        "\n",
        "    # Calculate sizes\n",
        "    size = weight.shape[0]\n",
        "    out_size = size * 2\n",
        "\n",
        "    # Set processing block size\n",
        "    Br = 8192\n",
        "    blocks = Br // block_size\n",
        "\n",
        "    # Calculate grid size\n",
        "    gsize = (triton.cdiv(out_size, Br))\n",
        "\n",
        "    # Optimize thread count based on GPU architecture\n",
        "    props = torch.cuda.get_device_properties(device)\n",
        "\n",
        "    # Adjust max threads per SM based on architecture\n",
        "    if props.major == 8:\n",
        "        if props.minor == 9:  # Ada Lovelace\n",
        "            max_th = 24 * props.multi_processor_count\n",
        "        elif props.minor == 0:  # Ampere\n",
        "            max_th = 32 * props.multi_processor_count\n",
        "    elif props.major == 7:\n",
        "        max_th = 16 * props.multi_processor_count  # Turing\n",
        "    else:\n",
        "        # Default for other architectures\n",
        "        max_th = 16 * props.multi_processor_count\n",
        "\n",
        "    # Wave-front scheduling optimization\n",
        "    resto = gsize % max_th\n",
        "    wave_sze = gsize if resto == 0 else gsize + (max_th - resto)\n",
        "\n",
        "    # Create output tensor with correct dtype handling\n",
        "    is_t4 = (props.major == 7 and props.minor == 5)\n",
        "    final_dtype = out_dtype if (out_dtype == torch.float16 or not is_t4) else torch.float16\n",
        "\n",
        "    w_out = torch.empty(real_shape, device=device,\n",
        "                        dtype=final_dtype,\n",
        "                        requires_grad=False)\n",
        "\n",
        "    # Launch kernel with wave-front scheduling\n",
        "    grid = lambda META: ((wave_sze,))\n",
        "\n",
        "    # Launch with optimized parameters\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight, w_out,\n",
        "        absmax, offset,\n",
        "        absmax2, code2,\n",
        "        block_size2, gsize, code, blocks, Br,\n",
        "        num_warps=16,\n",
        "        num_stages=1,\n",
        "        maxnreg=8192,\n",
        "    )\n",
        "\n",
        "    # Return with correct dtype\n",
        "    return w_out\n",
        "\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
      ],
      "metadata": {
        "id": "o9hxwwx0qx3M"
      },
      "id": "o9hxwwx0qx3M",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TEST IT BELOW:\n",
        "# test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "test_dequantize(unsloth_dequantize) / test_dequantize(your_dequantize_nf4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrO9u7dvpgDo",
        "outputId": "04caa075-6bb2-4e6a-c058-796a79f490a5"
      },
      "id": "hrO9u7dvpgDo",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2712974987482513"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQT2xiXypkGf"
      },
      "id": "CQT2xiXypkGf",
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}