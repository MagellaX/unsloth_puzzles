{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e161a016-8d1c-468c-84ea-4ab57ccdca76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e161a016-8d1c-468c-84ea-4ab57ccdca76",
        "outputId": "700d9226-6ccb-4518-ac66-3957f431c4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Collecting xformers==0.0.29\n",
            "  Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Collecting trl\n",
            "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.18.1-py3-none-any.whl (366 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, trl, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.0 trl-0.18.1 xformers-0.0.29\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.5.11-py3-none-any.whl.metadata (8.1 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.5.11-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\n",
            "Successfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.5.11\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.5.9-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.5.9-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.6/275.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.5.9\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install transformers tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "104f51e5-1fe9-430a-8d10-5bff70fe5fb6",
      "metadata": {
        "id": "104f51e5-1fe9-430a-8d10-5bff70fe5fb6"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=1e-1, rtol=1e-1)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d357bf28-e502-49b8-a6db-10564469c1ad",
      "metadata": {
        "id": "d357bf28-e502-49b8-a6db-10564469c1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765b45f9-17ce-457a-b705-b389eb28cc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-6109ecb06959>:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth.kernels.utils import fast_dequantize\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        # (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        # (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4656412-4dde-45a0-b73c-e15407a3f161",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4656412-4dde-45a0-b73c-e15407a3f161",
        "outputId": "6b64e284-27ea-43e5-b570-b4a1d80f8db0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1453566551208496"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f0f657f7-2c56-4ea5-9073-765a71b747f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f657f7-2c56-4ea5-9073-765a71b747f1",
        "outputId": "486b45ea-6656-498a-e02d-838373242867"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.372269868850708"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "\n",
        "# --- 1. Triton JIT Kernel (V13c - Cautious internal changes for speed) ---\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel( # V13 kernel base\n",
        "    w_ptr, abs_idx_ptr, offset_ptr, abs2_scales_ptr, code2_ptr, nf4_code_ptr, output_ptr,\n",
        "    TOTAL_ELEMENTS_IN_OUTPUT_TENSOR: tl.constexpr,\n",
        "    BLOCK_SIZE_BYTES_PER_CHUNK: tl.constexpr,\n",
        "    BLOCK_SIZE_ELEMENTS_PER_CHUNK: tl.constexpr,\n",
        "    NUM_GROUPS_PER_CHUNK: tl.constexpr,\n",
        "    ELEMENTS_PER_GROUP_CONST: tl.constexpr, # NF4 block size, e.g., 64\n",
        "    LOG2_L2_BLOCK_SIZE_CONST_KERNEL: tl.constexpr,\n",
        "    gsize_num_chunks: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    if pid >= gsize_num_chunks: return\n",
        "\n",
        "    log2_l2_block_size = LOG2_L2_BLOCK_SIZE_CONST_KERNEL\n",
        "\n",
        "    # --- Scale Calculation (identical to V13, this was numerically correct) ---\n",
        "    chunk_element_start_offset = pid * BLOCK_SIZE_ELEMENTS_PER_CHUNK\n",
        "    group_arange_local = tl.arange(0, NUM_GROUPS_PER_CHUNK)\n",
        "    absmax_group_indices_potential = (chunk_element_start_offset // ELEMENTS_PER_GROUP_CONST) + group_arange_local\n",
        "    group_mask = (absmax_group_indices_potential * ELEMENTS_PER_GROUP_CONST) < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "\n",
        "    quantized_absmax_indices = tl.load(abs_idx_ptr + absmax_group_indices_potential, mask=group_mask, other=0, eviction_policy=\"evict_first\")\n",
        "    dequantized_l1_scales = tl.load(code2_ptr + quantized_absmax_indices.to(tl.int32), mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "\n",
        "    absmax_l2_group_indices_potential = absmax_group_indices_potential >> log2_l2_block_size\n",
        "    l2_scales = tl.load(abs2_scales_ptr + absmax_l2_group_indices_potential, mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "\n",
        "    offset_val = tl.load(offset_ptr + 0)\n",
        "    intermediate_product_scales = l2_scales * dequantized_l1_scales\n",
        "    final_group_scales_masked = intermediate_product_scales + offset_val # Shape: (NUM_GROUPS_PER_CHUNK,)\n",
        "\n",
        "    # --- Weight Processing & Storing - Modified Section ---\n",
        "    # Iterate over each element this pid is responsible for\n",
        "    element_arange_pid_local = tl.arange(0, BLOCK_SIZE_ELEMENTS_PER_CHUNK)\n",
        "    global_element_indices = chunk_element_start_offset + element_arange_pid_local\n",
        "\n",
        "    # Overall mask for operations on these elements\n",
        "    element_op_mask = global_element_indices < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "\n",
        "    # Determine the byte and within-byte nibble position for each element\n",
        "    byte_index_global_for_element = global_element_indices // 2\n",
        "    is_low_nibble_flag = (global_element_indices % 2) != 0 # True if element is low nibble (odd index)\n",
        "\n",
        "    # Load the single byte containing the 2 nibbles for each element\n",
        "    # Mask this load to avoid reading out of bounds for w_ptr\n",
        "    # Each element needs its specific byte; this will involve redundant loads if not careful,\n",
        "    # but Triton's cache might help. For simplicity and correctness of masking:\n",
        "    packed_byte_for_element = tl.load(w_ptr + byte_index_global_for_element, mask=element_op_mask, other=0)\n",
        "\n",
        "    # Extract the relevant 4-bit index\n",
        "    nibble_shift = tl.where(is_low_nibble_flag, 0, 4)\n",
        "    quantized_idx_for_element = (packed_byte_for_element >> nibble_shift) & 0x0F\n",
        "\n",
        "    # Dequantize the 4-bit index using the NF4 codebook\n",
        "    # Mask this load if the element itself is out of bounds (already covered by element_op_mask if used)\n",
        "    dequant_val_for_element = tl.load(nf4_code_ptr + quantized_idx_for_element.to(tl.int32), mask=element_op_mask, other=0.0)\n",
        "\n",
        "    # Determine the correct scale for each element\n",
        "    group_idx_of_element_local = element_arange_pid_local // ELEMENTS_PER_GROUP_CONST\n",
        "\n",
        "    # Gather the scale for each element.\n",
        "    # final_group_scales_masked is shape (NUM_GROUPS_PER_CHUNK,)\n",
        "    # group_idx_of_element_local is shape (BLOCK_SIZE_ELEMENTS_PER_CHUNK,)\n",
        "    # We need to use a gather workaround if tl.gather is not available,\n",
        "    # otherwise, the reshape-broadcast-reshape used in the previous \"gather fix\" is correct.\n",
        "    # Assuming the reshape-broadcast-reshape for element_scales was correct and working:\n",
        "    scales_reshaped_for_broadcast = tl.reshape(final_group_scales_masked, (NUM_GROUPS_PER_CHUNK, 1))\n",
        "    scales_broadcasted_to_elements = tl.broadcast_to(scales_reshaped_for_broadcast, (NUM_GROUPS_PER_CHUNK, ELEMENTS_PER_GROUP_CONST))\n",
        "    element_scales_vector = tl.reshape(scales_broadcasted_to_elements, (BLOCK_SIZE_ELEMENTS_PER_CHUNK,))\n",
        "\n",
        "    # Apply element_op_mask to the gathered scales\n",
        "    final_scale_for_element = tl.where(element_op_mask, element_scales_vector, 0.0)\n",
        "\n",
        "    # Final scaled value for each element\n",
        "    scaled_element_output = dequant_val_for_element * final_scale_for_element\n",
        "\n",
        "    # Store the result\n",
        "    tl.store(output_ptr + global_element_indices, scaled_element_output, mask=element_op_mask)\n",
        "\n",
        "    return\n",
        "\n",
        "# --- Python Launcher for the V13c Kernel ---\n",
        "# (This remains largely the same as the V13 launcher, just calls the new kernel name)\n",
        "def _your_dequantize_nf4(\n",
        "    weight_data_actual,\n",
        "    quant_state_actual,\n",
        "    CHUNK_SIZE_ELEMENTS_Br: int,\n",
        "    num_warps_to_use: int,\n",
        "    num_stages_to_use: int\n",
        "    ):\n",
        "    device = weight_data_actual.device\n",
        "    output_shape = quant_state_actual.shape\n",
        "    total_elements_in_output = output_shape.numel()\n",
        "\n",
        "    if total_elements_in_output == 0:\n",
        "        props_local = torch.cuda.get_device_properties(device) if device.type == 'cuda' else None\n",
        "        is_t4_local = (props_local and props_local.major == 7 and props_local.minor == 5)\n",
        "        final_out_dtype_empty = quant_state_actual.dtype\n",
        "        if is_t4_local and quant_state_actual.dtype == torch.bfloat16: final_out_dtype_empty = torch.float16\n",
        "        return torch.empty(output_shape, device=device, dtype=final_out_dtype_empty)\n",
        "\n",
        "    props = torch.cuda.get_device_properties(device)\n",
        "    is_t4 = (props.major == 7 and props.minor == 5)\n",
        "    final_out_dtype = quant_state_actual.dtype\n",
        "    if is_t4 and quant_state_actual.dtype == torch.bfloat16: final_out_dtype = torch.float16\n",
        "\n",
        "    output_tensor = torch.empty(output_shape, dtype=final_out_dtype, device=device, requires_grad=False)\n",
        "\n",
        "    current_br = CHUNK_SIZE_ELEMENTS_Br\n",
        "    ELEMENTS_PER_GROUP_CONST = quant_state_actual.blocksize\n",
        "\n",
        "    if not hasattr(quant_state_actual, 'state2') or \\\n",
        "       not hasattr(quant_state_actual.state2, 'blocksize') or \\\n",
        "       quant_state_actual.state2.blocksize <= 0:\n",
        "        raise ValueError(\"quant_state.state2.blocksize is missing or invalid.\")\n",
        "    L2_BLOCK_SIZE_CONST_PYTHON = quant_state_actual.state2.blocksize\n",
        "    if not (L2_BLOCK_SIZE_CONST_PYTHON > 0 and (L2_BLOCK_SIZE_CONST_PYTHON & (L2_BLOCK_SIZE_CONST_PYTHON - 1) == 0)):\n",
        "        raise ValueError(f\"L2_BLOCK_SIZE_CONST_PYTHON must be a power of 2 and > 0. Got {L2_BLOCK_SIZE_CONST_PYTHON}\")\n",
        "\n",
        "    if total_elements_in_output < current_br:\n",
        "        current_br = triton.cdiv(total_elements_in_output, ELEMENTS_PER_GROUP_CONST) * ELEMENTS_PER_GROUP_CONST \\\n",
        "            if total_elements_in_output > ELEMENTS_PER_GROUP_CONST else ELEMENTS_PER_GROUP_CONST\n",
        "    if current_br == 0 and total_elements_in_output > 0 : current_br = ELEMENTS_PER_GROUP_CONST\n",
        "    elif current_br < ELEMENTS_PER_GROUP_CONST and total_elements_in_output > 0: current_br = ELEMENTS_PER_GROUP_CONST\n",
        "\n",
        "    CHUNK_SIZE_BYTES_Br_half = current_br // 2\n",
        "    NUM_GROUPS_PER_CHUNK_Br_div_64 = current_br // ELEMENTS_PER_GROUP_CONST\n",
        "\n",
        "    if NUM_GROUPS_PER_CHUNK_Br_div_64 == 0 and total_elements_in_output > 0:\n",
        "        if current_br < ELEMENTS_PER_GROUP_CONST: current_br = ELEMENTS_PER_GROUP_CONST\n",
        "        CHUNK_SIZE_BYTES_Br_half = current_br // 2\n",
        "        NUM_GROUPS_PER_CHUNK_Br_div_64 = current_br // ELEMENTS_PER_GROUP_CONST\n",
        "        if NUM_GROUPS_PER_CHUNK_Br_div_64 == 0 :\n",
        "            raise ValueError(f\"Cannot form groups. Effective Br: {current_br}, Total Elem: {total_elements_in_output}, Group Size: {ELEMENTS_PER_GROUP_CONST}\")\n",
        "\n",
        "    LOG2_L2_BLOCK_SIZE_CONST_PYTHON = int(math.log2(L2_BLOCK_SIZE_CONST_PYTHON))\n",
        "\n",
        "    gsize_num_chunks_val = triton.cdiv(total_elements_in_output, current_br)\n",
        "    if gsize_num_chunks_val == 0 and total_elements_in_output > 0:\n",
        "        gsize_num_chunks_val = 1\n",
        "\n",
        "    grid = (gsize_num_chunks_val,)\n",
        "\n",
        "    const_args_for_kernel = {\n",
        "        'TOTAL_ELEMENTS_IN_OUTPUT_TENSOR': total_elements_in_output,\n",
        "        'BLOCK_SIZE_BYTES_PER_CHUNK': CHUNK_SIZE_BYTES_Br_half, # May not be directly used by kernel if logic changes\n",
        "        'BLOCK_SIZE_ELEMENTS_PER_CHUNK': current_br,\n",
        "        'NUM_GROUPS_PER_CHUNK': NUM_GROUPS_PER_CHUNK_Br_div_64,\n",
        "        'ELEMENTS_PER_GROUP_CONST': ELEMENTS_PER_GROUP_CONST,\n",
        "        'LOG2_L2_BLOCK_SIZE_CONST_KERNEL': LOG2_L2_BLOCK_SIZE_CONST_PYTHON,\n",
        "        'gsize_num_chunks': gsize_num_chunks_val\n",
        "    }\n",
        "\n",
        "    # Make sure to call the correct kernel name\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight_data_actual, quant_state_actual.absmax, quant_state_actual.offset,\n",
        "        quant_state_actual.state2.absmax, quant_state_actual.state2.code, quant_state_actual.code,\n",
        "        output_tensor,\n",
        "        **const_args_for_kernel,\n",
        "        num_warps=num_warps_to_use,\n",
        "        num_stages=num_stages_to_use\n",
        "    )\n",
        "    return output_tensor\n",
        "\n",
        "# --- 3. Top-Level Wrapper (Set your best launch parameters) ---\n",
        "def your_dequantize_nf4(weight_param):\n",
        "    OPTIMIZED_BR = 8192\n",
        "    OPTIMIZED_WARPS = 16\n",
        "    OPTIMIZED_STAGES = 1 # Start with parameters that were fast previously\n",
        "\n",
        "    if not hasattr(weight_param, 'weight') or \\\n",
        "       not hasattr(weight_param.weight, 'data') or \\\n",
        "       not hasattr(weight_param.weight, 'quant_state'):\n",
        "        raise ValueError(\"Input 'weight_param' is not the expected Linear4bit layer format or lacks necessary attributes.\")\n",
        "    weight_data_actual = weight_param.weight.data\n",
        "    quant_state_actual = weight_param.weight.quant_state\n",
        "    return _your_dequantize_nf4(\n",
        "        weight_data_actual, quant_state_actual,\n",
        "        CHUNK_SIZE_ELEMENTS_Br=OPTIMIZED_BR,\n",
        "        num_warps_to_use=OPTIMIZED_WARPS,\n",
        "        num_stages_to_use=OPTIMIZED_STAGES\n",
        "    )"
      ],
      "metadata": {
        "id": "dd8zpxrGXXb8"
      },
      "id": "dd8zpxrGXXb8",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TEST IT BELOW:\n",
        "# test_dequantize(your_dequantize_nf4)\n",
        "\n",
        "### CALCULATE SPEEDUP (hopefully 1.15x faster or more)\n",
        "test_dequantize(unsloth_dequantize) / test_dequantize(your_dequantize_nf4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrO9u7dvpgDo",
        "outputId": "e2a602b7-deea-4b34-ed26-e8e5f2698361"
      },
      "id": "hrO9u7dvpgDo",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1819698468989508"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "from transformers import set_seed\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "# Ensure fast_dequantize is accessible from your Unsloth installation\n",
        "from unsloth.kernels.utils import fast_dequantize # Assuming this path is correct\n",
        "\n",
        "# --- 1. Triton JIT Kernel (V13c - Fast Version with element-centric processing) ---\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(\n",
        "    w_ptr, abs_idx_ptr, offset_ptr, abs2_scales_ptr, code2_ptr, nf4_code_ptr, output_ptr,\n",
        "    TOTAL_ELEMENTS_IN_OUTPUT_TENSOR: tl.constexpr,\n",
        "    BLOCK_SIZE_BYTES_PER_CHUNK: tl.constexpr, # Not directly used in V13c main loop, but part of const_args\n",
        "    BLOCK_SIZE_ELEMENTS_PER_CHUNK: tl.constexpr, # This is BR_CHUNK_SIZE_ELEMENTS\n",
        "    NUM_GROUPS_PER_CHUNK: tl.constexpr,\n",
        "    ELEMENTS_PER_GROUP_CONST: tl.constexpr, # NF4 block size, e.g., 64\n",
        "    LOG2_L2_BLOCK_SIZE_CONST_KERNEL: tl.constexpr,\n",
        "    gsize_num_chunks: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    if pid >= gsize_num_chunks: return\n",
        "\n",
        "    log2_l2_block_size = LOG2_L2_BLOCK_SIZE_CONST_KERNEL\n",
        "\n",
        "    # --- Scale Calculation (identical to V13) ---\n",
        "    chunk_element_start_offset = pid * BLOCK_SIZE_ELEMENTS_PER_CHUNK\n",
        "    group_arange_local = tl.arange(0, NUM_GROUPS_PER_CHUNK)\n",
        "    absmax_group_indices_potential = (chunk_element_start_offset // ELEMENTS_PER_GROUP_CONST) + group_arange_local\n",
        "    group_mask = (absmax_group_indices_potential * ELEMENTS_PER_GROUP_CONST) < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "\n",
        "    quantized_absmax_indices = tl.load(abs_idx_ptr + absmax_group_indices_potential, mask=group_mask, other=0, eviction_policy=\"evict_first\")\n",
        "    dequantized_l1_scales = tl.load(code2_ptr + quantized_absmax_indices.to(tl.int32), mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "\n",
        "    absmax_l2_group_indices_potential = absmax_group_indices_potential >> log2_l2_block_size\n",
        "    l2_scales = tl.load(abs2_scales_ptr + absmax_l2_group_indices_potential, mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "\n",
        "    offset_val = tl.load(offset_ptr + 0)\n",
        "    intermediate_product_scales = l2_scales * dequantized_l1_scales\n",
        "    final_group_scales_masked = intermediate_product_scales + offset_val # Shape: (NUM_GROUPS_PER_CHUNK,)\n",
        "\n",
        "    # --- Weight Processing & Storing (V13c element-centric approach) ---\n",
        "    element_arange_pid_local = tl.arange(0, BLOCK_SIZE_ELEMENTS_PER_CHUNK) # Iterates 0 to BR_CHUNK_SIZE_ELEMENTS-1\n",
        "    global_element_indices = chunk_element_start_offset + element_arange_pid_local\n",
        "    element_op_mask = global_element_indices < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "\n",
        "    byte_index_global_for_element = global_element_indices // 2\n",
        "    is_low_nibble_flag = (global_element_indices % 2) != 0\n",
        "\n",
        "    # Load the single byte that contains the 2 nibbles for each element.\n",
        "    # Masked to prevent out-of-bounds reads for w_ptr.\n",
        "    packed_byte_for_element = tl.load(w_ptr + byte_index_global_for_element, mask=element_op_mask, other=0)\n",
        "\n",
        "    nibble_shift = tl.where(is_low_nibble_flag, 0, 4)\n",
        "    quantized_idx_for_element = (packed_byte_for_element >> nibble_shift) & 0x0F\n",
        "\n",
        "    dequant_val_for_element = tl.load(nf4_code_ptr + quantized_idx_for_element.to(tl.int32), mask=element_op_mask, other=0.0)\n",
        "\n",
        "    # Gather scale for each element (using reshape-broadcast-reshape as tl.gather workaround)\n",
        "    group_idx_of_element_local = element_arange_pid_local // ELEMENTS_PER_GROUP_CONST\n",
        "    scales_reshaped_for_broadcast = tl.reshape(final_group_scales_masked, (NUM_GROUPS_PER_CHUNK, 1))\n",
        "    scales_broadcasted_to_elements = tl.broadcast_to(scales_reshaped_for_broadcast, (NUM_GROUPS_PER_CHUNK, ELEMENTS_PER_GROUP_CONST))\n",
        "    element_scales_vector = tl.reshape(scales_broadcasted_to_elements, (BLOCK_SIZE_ELEMENTS_PER_CHUNK,))\n",
        "    final_scale_for_element = tl.where(element_op_mask, element_scales_vector, 0.0)\n",
        "\n",
        "    scaled_element_output = dequant_val_for_element * final_scale_for_element\n",
        "\n",
        "    tl.store(output_ptr + global_element_indices, scaled_element_output, mask=element_op_mask)\n",
        "    return\n",
        "\n",
        "# --- 2. Python Launcher for the V13c Kernel ---\n",
        "def _your_dequantize_nf4(\n",
        "    weight_data_actual,\n",
        "    quant_state_actual,\n",
        "    OPTIMIZED_BR: int,\n",
        "    OPTIMIZED_WARPS: int,\n",
        "    OPTIMIZED_STAGES: int\n",
        "    ):\n",
        "    device = weight_data_actual.device\n",
        "    output_shape = quant_state_actual.shape\n",
        "    total_elements_in_output = output_shape.numel()\n",
        "\n",
        "    if total_elements_in_output == 0:\n",
        "        props_local = torch.cuda.get_device_properties(device) if device.type == 'cuda' else None; is_t4_local = (props_local and props_local.major == 7 and props_local.minor == 5)\n",
        "        final_out_dtype_empty = quant_state_actual.dtype;\n",
        "        if is_t4_local and quant_state_actual.dtype == torch.bfloat16: final_out_dtype_empty = torch.float16\n",
        "        return torch.empty(output_shape, device=device, dtype=final_out_dtype_empty)\n",
        "\n",
        "    props = torch.cuda.get_device_properties(device)\n",
        "    is_t4 = (props.major == 7 and props.minor == 5)\n",
        "    final_out_dtype = quant_state_actual.dtype\n",
        "    if is_t4 and quant_state_actual.dtype == torch.bfloat16: final_out_dtype = torch.float16\n",
        "\n",
        "    output_tensor = torch.empty(output_shape, dtype=final_out_dtype, device=device, requires_grad=False)\n",
        "\n",
        "    current_br_chunk_size_elements = OPTIMIZED_BR # This is BLOCK_SIZE_ELEMENTS_PER_CHUNK for kernel\n",
        "    nf4_block_size_elements = quant_state_actual.blocksize # This is ELEMENTS_PER_GROUP_CONST for kernel\n",
        "\n",
        "    if not hasattr(quant_state_actual, 'state2') or \\\n",
        "       not hasattr(quant_state_actual.state2, 'blocksize') or \\\n",
        "       quant_state_actual.state2.blocksize <= 0:\n",
        "        raise ValueError(\"quant_state.state2.blocksize is missing or invalid.\")\n",
        "    block_size2_absmax_val = quant_state_actual.state2.blocksize\n",
        "    if not (block_size2_absmax_val > 0 and (block_size2_absmax_val & (block_size2_absmax_val - 1) == 0)):\n",
        "        raise ValueError(f\"block_size2_absmax_val must be a power of 2 and > 0. Got {block_size2_absmax_val}\")\n",
        "\n",
        "    # Adjust current_br_chunk_size_elements for small tensors\n",
        "    if total_elements_in_output < current_br_chunk_size_elements:\n",
        "        current_br_chunk_size_elements = triton.cdiv(total_elements_in_output, nf4_block_size_elements) * nf4_block_size_elements \\\n",
        "            if total_elements_in_output > nf4_block_size_elements else nf4_block_size_elements\n",
        "    if current_br_chunk_size_elements == 0 and total_elements_in_output > 0 : current_br_chunk_size_elements = nf4_block_size_elements\n",
        "    elif current_br_chunk_size_elements < nf4_block_size_elements and total_elements_in_output > 0: current_br_chunk_size_elements = nf4_block_size_elements\n",
        "\n",
        "    num_nf4_blocks_per_br_chunk_val = current_br_chunk_size_elements // nf4_block_size_elements\n",
        "    if num_nf4_blocks_per_br_chunk_val == 0 and total_elements_in_output > 0:\n",
        "        # This implies current_br_chunk_size_elements was < nf4_block_size_elements.\n",
        "        # Ensure current_br_chunk_size_elements is at least nf4_block_size_elements.\n",
        "        current_br_chunk_size_elements = nf4_block_size_elements\n",
        "        num_nf4_blocks_per_br_chunk_val = 1 # Must have at least one group per chunk if processing data\n",
        "\n",
        "    # BLOCK_SIZE_BYTES_PER_CHUNK is based on current_br_chunk_size_elements for V13c element-centric logic\n",
        "    # However, the V13c kernel doesn't directly use BLOCK_SIZE_BYTES_PER_CHUNK for a bulk load anymore.\n",
        "    # It's still passed as a constexpr, so we calculate it for completeness.\n",
        "    block_size_bytes_per_chunk_val = current_br_chunk_size_elements // 2\n",
        "\n",
        "\n",
        "    gsize_val = triton.cdiv(total_elements_in_output, current_br_chunk_size_elements)\n",
        "    if gsize_val == 0 and total_elements_in_output > 0:\n",
        "        gsize_val = 1\n",
        "\n",
        "    grid = (gsize_val,)\n",
        "\n",
        "    const_args_for_kernel = {\n",
        "        'TOTAL_ELEMENTS_IN_OUTPUT_TENSOR': total_elements_in_output,\n",
        "        'BLOCK_SIZE_BYTES_PER_CHUNK': block_size_bytes_per_chunk_val, # Used by V13 original, less critical in V13c loop\n",
        "        'BLOCK_SIZE_ELEMENTS_PER_CHUNK': current_br_chunk_size_elements,\n",
        "        'NUM_GROUPS_PER_CHUNK': num_nf4_blocks_per_br_chunk_val,\n",
        "        'ELEMENTS_PER_GROUP_CONST': nf4_block_size_elements,\n",
        "        'LOG2_L2_BLOCK_SIZE_CONST_KERNEL': int(math.log2(block_size2_absmax_val)),\n",
        "        'gsize_num_chunks': gsize_val # Kernel uses this as gsize_num_chunks\n",
        "    }\n",
        "\n",
        "    # Kernel call for V13c\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight_data_actual,             # w_ptr\n",
        "        quant_state_actual.absmax,      # abs_idx_ptr (L1 uint8 indices)\n",
        "        quant_state_actual.offset,      # offset_ptr\n",
        "        quant_state_actual.state2.absmax, # abs2_scales_ptr (L2 float32 scales)\n",
        "        quant_state_actual.state2.code, # code2_ptr (L2 codebook for L1 absmax)\n",
        "        quant_state_actual.code,        # nf4_code_ptr (NF4 codebook for weights)\n",
        "        output_tensor,                  # output_ptr\n",
        "        # **const_args_for_kernel, # Pass them as keywords to match kernel def\n",
        "        TOTAL_ELEMENTS_IN_OUTPUT_TENSOR=total_elements_in_output,\n",
        "        BLOCK_SIZE_BYTES_PER_CHUNK=block_size_bytes_per_chunk_val,\n",
        "        BLOCK_SIZE_ELEMENTS_PER_CHUNK=current_br_chunk_size_elements,\n",
        "        NUM_GROUPS_PER_CHUNK=num_nf4_blocks_per_br_chunk_val,\n",
        "        ELEMENTS_PER_GROUP_CONST=nf4_block_size_elements,\n",
        "        LOG2_L2_BLOCK_SIZE_CONST_KERNEL=int(math.log2(block_size2_absmax_val)),\n",
        "        gsize_num_chunks=gsize_val,\n",
        "        num_warps=OPTIMIZED_WARPS,\n",
        "        num_stages=OPTIMIZED_STAGES\n",
        "    )\n",
        "    return output_tensor\n",
        "\n",
        "# --- 3. Top-Level Wrapper ---\n",
        "def your_dequantize_nf4(weight_param):\n",
        "    # Using parameters that achieved 1.32x speedup\n",
        "    OPTIMIZED_BR = 8192\n",
        "    OPTIMIZED_WARPS = 16\n",
        "    OPTIMIZED_STAGES = 1\n",
        "\n",
        "    if not hasattr(weight_param, 'weight') or \\\n",
        "       not hasattr(weight_param.weight, 'data') or \\\n",
        "       not hasattr(weight_param.weight, 'quant_state'):\n",
        "        raise ValueError(\"Input 'weight_param' is not the expected Linear4bit layer format or lacks necessary attributes.\")\n",
        "\n",
        "    packed_weight_data = weight_param.weight.data\n",
        "    quant_state_object = weight_param.weight.quant_state\n",
        "\n",
        "    return _your_dequantize_nf4(\n",
        "        packed_weight_data, quant_state_object,\n",
        "        OPTIMIZED_BR     = OPTIMIZED_BR,\n",
        "        OPTIMIZED_WARPS  = OPTIMIZED_WARPS,\n",
        "        OPTIMIZED_STAGES = OPTIMIZED_STAGES\n",
        "    )\n",
        "\n",
        "# --- 4. Reference Dequantization Function ---\n",
        "def unsloth_dequantize_for_test(weight_param):\n",
        "    if 'fast_dequantize' not in globals():\n",
        "        try:\n",
        "            from unsloth.kernels.utils import fast_dequantize as fd_temp\n",
        "            globals()['fast_dequantize'] = fd_temp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please ensure 'fast_dequantize' from 'unsloth.kernels.utils' is available.\")\n",
        "    return fast_dequantize(weight_param.weight, weight_param.weight.quant_state)\n",
        "\n",
        "# --- 5. Numerical Correctness Check Function (Same as before) ---\n",
        "def check_numerical_correctness_one_shot(\n",
        "    your_kernel_func,\n",
        "    reference_func,\n",
        "    hidden_size=2048,\n",
        "    intermediate_size=8192,\n",
        "    dtype_str=\"fp16\",\n",
        "    seed=3407,\n",
        "    atol_strict=1e-3,\n",
        "    rtol_strict=1e-3\n",
        "    ):\n",
        "    print(f\"\\n--- Numerical Correctness Check ---\")\n",
        "    print(f\"Your Kernel: {your_kernel_func.__name__} (Testing V13c element-centric kernel)\")\n",
        "    print(f\"Reference  : {reference_func.__name__}\")\n",
        "    print(f\"Parameters : hidden_size={hidden_size}, intermediate_size={intermediate_size}, dtype={dtype_str}, seed={seed}\")\n",
        "    print(f\"Tolerances : atol={atol_strict}, rtol={rtol_strict}\")\n",
        "\n",
        "    set_seed(seed)\n",
        "    torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    current_dtype = torch.float16\n",
        "    quant_state_target_dtype = torch.float16\n",
        "\n",
        "    if dtype_str == \"bf16\":\n",
        "        quant_state_target_dtype = torch.bfloat16\n",
        "        major_version, minor_version = torch.cuda.get_device_capability()\n",
        "        if major_version >= 8:\n",
        "            current_dtype = torch.bfloat16\n",
        "        else:\n",
        "            print(f\"Note: bf16 requested. GPU (SM {major_version}.{minor_version}) will use fp16 for Linear4bit compute_dtype. Kernel output path for T4 (if applicable) will be fp16.\")\n",
        "            current_dtype = torch.float16\n",
        "\n",
        "    try:\n",
        "        layer_to_test = Linear4bit(\n",
        "            hidden_size,\n",
        "            intermediate_size,\n",
        "            bias=None,\n",
        "            compute_dtype=current_dtype,\n",
        "            compress_statistics=True,\n",
        "            quant_type=\"nf4\",\n",
        "        ).to(\"cuda\")\n",
        "        layer_to_test.weight.quant_state.dtype = quant_state_target_dtype\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Linear4bit layer setup: {e}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        W_your_kernel = your_kernel_func(layer_to_test)\n",
        "    except Exception as e:\n",
        "        print(f\"Error running YOUR dequantization kernel: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        W_reference = reference_func(layer_to_test)\n",
        "    except Exception as e:\n",
        "        print(f\"Error running REFERENCE dequantization function: {e}\")\n",
        "        return False\n",
        "\n",
        "    if W_your_kernel.device != W_reference.device:\n",
        "        W_reference = W_reference.to(W_your_kernel.device)\n",
        "    if W_your_kernel.dtype != W_reference.dtype:\n",
        "        print(f\"Casting reference tensor from {W_reference.dtype} to {W_your_kernel.dtype} for comparison.\")\n",
        "        W_reference = W_reference.to(W_your_kernel.dtype)\n",
        "\n",
        "    print(f\"\\nComparing outputs (Your Kernel Dtype: {W_your_kernel.dtype}, Reference Dtype After Cast: {W_reference.dtype})...\")\n",
        "    try:\n",
        "        torch.testing.assert_close(W_your_kernel, W_reference, atol=atol_strict, rtol=rtol_strict, check_stride=False)\n",
        "        print(f\"SUCCESS: Outputs are numerically close within atol={atol_strict}, rtol={rtol_strict}.\")\n",
        "        return True\n",
        "    except AssertionError as e:\n",
        "        print(f\"FAILURE: Outputs are NOT numerically close within atol={atol_strict}, rtol={rtol_strict}.\")\n",
        "        abs_diff = torch.abs(W_your_kernel - W_reference)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        mean_abs_diff = torch.mean(abs_diff)\n",
        "        idx_max_abs_diff_flat = torch.argmax(abs_diff.flatten())\n",
        "        val_your_kernel_at_max_diff = W_your_kernel.flatten()[idx_max_abs_diff_flat]\n",
        "        val_reference_at_max_diff = W_reference.flatten()[idx_max_abs_diff_flat]\n",
        "        print(f\"  Max absolute difference: {max_abs_diff.item():.8e}\")\n",
        "        print(f\"  Mean absolute difference: {mean_abs_diff.item():.8e}\")\n",
        "        print(f\"  Value from your kernel at max diff flat_index ({idx_max_abs_diff_flat.item()}): {val_your_kernel_at_max_diff.item():.8f}\")\n",
        "        print(f\"  Value from reference at max diff flat_index ({idx_max_abs_diff_flat.item()}): {val_reference_at_max_diff.item():.8f}\")\n",
        "        threshold_context = 0.01\n",
        "        num_elements_over_threshold = torch.sum(abs_diff > threshold_context).item()\n",
        "        total_elements_val = W_your_kernel.numel()\n",
        "        percentage_over_threshold = (num_elements_over_threshold / total_elements_val) * 100 if total_elements_val > 0 else 0\n",
        "        print(f\"  Number of elements with abs_diff > {threshold_context}: {num_elements_over_threshold} / {total_elements_val} ({percentage_over_threshold:.4f}%)\")\n",
        "        if 0 < num_elements_over_threshold < 20:\n",
        "            print(f\"  First few flat_indices where abs_diff > {threshold_context}: {torch.nonzero(abs_diff.flatten() > threshold_context).squeeze()[:5].tolist()}\")\n",
        "        return False\n",
        "\n",
        "# --- 6. Example Usage for Numerical Correctness Test ---\n",
        "print(\"Running numerical correctness check for V13c Hybrid Kernel (fp16)...\")\n",
        "is_correct_fp16_v13c = check_numerical_correctness_one_shot(\n",
        "    your_kernel_func=your_dequantize_nf4,\n",
        "    reference_func=unsloth_dequantize_for_test,\n",
        "    hidden_size=2048,\n",
        "    intermediate_size=8192,\n",
        "    dtype_str=\"fp16\",\n",
        "    seed=3407,\n",
        "    atol_strict=1e-3,\n",
        "    rtol_strict=1e-3\n",
        ")\n",
        "print(f\"Overall numerical correctness for V13c Hybrid Kernel fp16 (strict check): {'PASSED' if is_correct_fp16_v13c else 'FAILED'}\")\n",
        "\n",
        "print(\"\\nRunning numerical correctness check for V13c Hybrid Kernel (bf16)...\")\n",
        "is_correct_bf16_v13c = check_numerical_correctness_one_shot(\n",
        "    your_kernel_func=your_dequantize_nf4,\n",
        "    reference_func=unsloth_dequantize_for_test,\n",
        "    hidden_size=1024,\n",
        "    intermediate_size=4096,\n",
        "    dtype_str=\"bf16\",\n",
        "    seed=3409,\n",
        "    atol_strict=1e-2,\n",
        "    rtol_strict=1e-2\n",
        ")\n",
        "print(f\"Overall numerical correctness for V13c Hybrid Kernel bf16 input (strict check): {'PASSED' if is_correct_bf16_v13c else 'FAILED'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wKv-9JoQCJ6",
        "outputId": "ea55e191-88b0-4ee4-d466-f5119f2cc467"
      },
      "id": "8wKv-9JoQCJ6",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running numerical correctness check for V13c Hybrid Kernel (fp16)...\n",
            "\n",
            "--- Numerical Correctness Check ---\n",
            "Your Kernel: your_dequantize_nf4 (Testing V13c element-centric kernel)\n",
            "Reference  : unsloth_dequantize_for_test\n",
            "Parameters : hidden_size=2048, intermediate_size=8192, dtype=fp16, seed=3407\n",
            "Tolerances : atol=0.001, rtol=0.001\n",
            "\n",
            "Comparing outputs (Your Kernel Dtype: torch.float16, Reference Dtype After Cast: torch.float16)...\n",
            "SUCCESS: Outputs are numerically close within atol=0.001, rtol=0.001.\n",
            "Overall numerical correctness for V13c Hybrid Kernel fp16 (strict check): PASSED\n",
            "\n",
            "Running numerical correctness check for V13c Hybrid Kernel (bf16)...\n",
            "\n",
            "--- Numerical Correctness Check ---\n",
            "Your Kernel: your_dequantize_nf4 (Testing V13c element-centric kernel)\n",
            "Reference  : unsloth_dequantize_for_test\n",
            "Parameters : hidden_size=1024, intermediate_size=4096, dtype=bf16, seed=3409\n",
            "Tolerances : atol=0.01, rtol=0.01\n",
            "Note: bf16 requested. GPU (SM 7.5) will use fp16 for Linear4bit compute_dtype. Kernel output path for T4 (if applicable) will be fp16.\n",
            "Casting reference tensor from torch.bfloat16 to torch.float16 for comparison.\n",
            "\n",
            "Comparing outputs (Your Kernel Dtype: torch.float16, Reference Dtype After Cast: torch.float16)...\n",
            "SUCCESS: Outputs are numerically close within atol=0.01, rtol=0.01.\n",
            "Overall numerical correctness for V13c Hybrid Kernel bf16 input (strict check): PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "import torch._dynamo as dynamo\n",
        "\n",
        "# --- 1. Your V13c Kernel and Launcher (as provided) ---\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel( # V13 kernel base\n",
        "    w_ptr, abs_idx_ptr, offset_ptr, abs2_scales_ptr, code2_ptr, nf4_code_ptr, output_ptr,\n",
        "    TOTAL_ELEMENTS_IN_OUTPUT_TENSOR: tl.constexpr,\n",
        "    BLOCK_SIZE_BYTES_PER_CHUNK: tl.constexpr,\n",
        "    BLOCK_SIZE_ELEMENTS_PER_CHUNK: tl.constexpr,\n",
        "    NUM_GROUPS_PER_CHUNK: tl.constexpr,\n",
        "    ELEMENTS_PER_GROUP_CONST: tl.constexpr, # NF4 block size, e.g., 64\n",
        "    LOG2_L2_BLOCK_SIZE_CONST_KERNEL: tl.constexpr,\n",
        "    gsize_num_chunks: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    if pid >= gsize_num_chunks: return\n",
        "    log2_l2_block_size = LOG2_L2_BLOCK_SIZE_CONST_KERNEL\n",
        "    chunk_element_start_offset = pid * BLOCK_SIZE_ELEMENTS_PER_CHUNK\n",
        "    group_arange_local = tl.arange(0, NUM_GROUPS_PER_CHUNK)\n",
        "    absmax_group_indices_potential = (chunk_element_start_offset // ELEMENTS_PER_GROUP_CONST) + group_arange_local\n",
        "    group_mask = (absmax_group_indices_potential * ELEMENTS_PER_GROUP_CONST) < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "    quantized_absmax_indices = tl.load(abs_idx_ptr + absmax_group_indices_potential, mask=group_mask, other=0, eviction_policy=\"evict_first\")\n",
        "    dequantized_l1_scales = tl.load(code2_ptr + quantized_absmax_indices.to(tl.int32), mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "    absmax_l2_group_indices_potential = absmax_group_indices_potential >> log2_l2_block_size\n",
        "    l2_scales = tl.load(abs2_scales_ptr + absmax_l2_group_indices_potential, mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "    offset_val = tl.load(offset_ptr + 0)\n",
        "    intermediate_product_scales = l2_scales * dequantized_l1_scales\n",
        "    final_group_scales_masked = intermediate_product_scales + offset_val\n",
        "    element_arange_pid_local = tl.arange(0, BLOCK_SIZE_ELEMENTS_PER_CHUNK)\n",
        "    global_element_indices = chunk_element_start_offset + element_arange_pid_local\n",
        "    element_op_mask = global_element_indices < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "    byte_index_global_for_element = global_element_indices // 2\n",
        "    is_low_nibble_flag = (global_element_indices % 2) != 0\n",
        "    packed_byte_for_element = tl.load(w_ptr + byte_index_global_for_element, mask=element_op_mask, other=0)\n",
        "    nibble_shift = tl.where(is_low_nibble_flag, 0, 4)\n",
        "    quantized_idx_for_element = (packed_byte_for_element >> nibble_shift) & 0x0F\n",
        "    dequant_val_for_element = tl.load(nf4_code_ptr + quantized_idx_for_element.to(tl.int32), mask=element_op_mask, other=0.0)\n",
        "    scales_reshaped_for_broadcast = tl.reshape(final_group_scales_masked, (NUM_GROUPS_PER_CHUNK, 1))\n",
        "    scales_broadcasted_to_elements = tl.broadcast_to(scales_reshaped_for_broadcast, (NUM_GROUPS_PER_CHUNK, ELEMENTS_PER_GROUP_CONST))\n",
        "    element_scales_vector = tl.reshape(scales_broadcasted_to_elements, (BLOCK_SIZE_ELEMENTS_PER_CHUNK,))\n",
        "    final_scale_for_element = tl.where(element_op_mask, element_scales_vector, 0.0)\n",
        "    scaled_element_output = dequant_val_for_element * final_scale_for_element\n",
        "    tl.store(output_ptr + global_element_indices, scaled_element_output, mask=element_op_mask)\n",
        "    return\n",
        "\n",
        "def _your_dequantize_nf4(\n",
        "    weight_data_actual, quant_state_actual,\n",
        "    CHUNK_SIZE_ELEMENTS_Br: int, num_warps_to_use: int, num_stages_to_use: int\n",
        "):\n",
        "    device = weight_data_actual.device\n",
        "    output_shape = quant_state_actual.shape\n",
        "    total_elements_in_output = output_shape.numel()\n",
        "    if total_elements_in_output == 0:\n",
        "        return torch.empty(output_shape, device=device, dtype=quant_state_actual.dtype)\n",
        "    props = torch.cuda.get_device_properties(device)\n",
        "    is_t4 = (props.major == 7 and props.minor == 5)\n",
        "    final_out_dtype = quant_state_actual.dtype\n",
        "    if is_t4 and quant_state_actual.dtype == torch.bfloat16: final_out_dtype = torch.float16\n",
        "    output_tensor = torch.empty(output_shape, dtype=final_out_dtype, device=device, requires_grad=False)\n",
        "    current_br = CHUNK_SIZE_ELEMENTS_Br\n",
        "    ELEMENTS_PER_GROUP_CONST = quant_state_actual.blocksize\n",
        "    L2_BLOCK_SIZE_CONST_PYTHON = quant_state_actual.state2.blocksize\n",
        "    if not (L2_BLOCK_SIZE_CONST_PYTHON > 0 and (L2_BLOCK_SIZE_CONST_PYTHON & (L2_BLOCK_SIZE_CONST_PYTHON - 1) == 0)):\n",
        "        raise ValueError(f\"L2_BLOCK_SIZE_CONST_PYTHON must be a power of 2 and > 0. Got {L2_BLOCK_SIZE_CONST_PYTHON}\")\n",
        "    if total_elements_in_output < current_br:\n",
        "        current_br = triton.cdiv(total_elements_in_output, ELEMENTS_PER_GROUP_CONST) * ELEMENTS_PER_GROUP_CONST \\\n",
        "            if total_elements_in_output > ELEMENTS_PER_GROUP_CONST else ELEMENTS_PER_GROUP_CONST\n",
        "    if current_br == 0 and total_elements_in_output > 0 : current_br = ELEMENTS_PER_GROUP_CONST\n",
        "    elif current_br < ELEMENTS_PER_GROUP_CONST and total_elements_in_output > 0: current_br = ELEMENTS_PER_GROUP_CONST\n",
        "    CHUNK_SIZE_BYTES_Br_half = current_br // 2\n",
        "    NUM_GROUPS_PER_CHUNK_Br_div_64 = current_br // ELEMENTS_PER_GROUP_CONST\n",
        "    if NUM_GROUPS_PER_CHUNK_Br_div_64 == 0 and total_elements_in_output > 0:\n",
        "        if current_br < ELEMENTS_PER_GROUP_CONST: current_br = ELEMENTS_PER_GROUP_CONST\n",
        "        CHUNK_SIZE_BYTES_Br_half = current_br // 2\n",
        "        NUM_GROUPS_PER_CHUNK_Br_div_64 = current_br // ELEMENTS_PER_GROUP_CONST\n",
        "        if NUM_GROUPS_PER_CHUNK_Br_div_64 == 0 :\n",
        "            raise ValueError(f\"Cannot form groups. Effective Br: {current_br}, Total Elem: {total_elements_in_output}, Group Size: {ELEMENTS_PER_GROUP_CONST}\")\n",
        "    LOG2_L2_BLOCK_SIZE_CONST_PYTHON = int(math.log2(L2_BLOCK_SIZE_CONST_PYTHON))\n",
        "    gsize_num_chunks_val = triton.cdiv(total_elements_in_output, current_br)\n",
        "    if gsize_num_chunks_val == 0 and total_elements_in_output > 0:\n",
        "        gsize_num_chunks_val = 1\n",
        "    grid = (gsize_num_chunks_val,)\n",
        "    const_args_for_kernel = {\n",
        "        'TOTAL_ELEMENTS_IN_OUTPUT_TENSOR': total_elements_in_output,\n",
        "        'BLOCK_SIZE_BYTES_PER_CHUNK': CHUNK_SIZE_BYTES_Br_half,\n",
        "        'BLOCK_SIZE_ELEMENTS_PER_CHUNK': current_br,\n",
        "        'NUM_GROUPS_PER_CHUNK': NUM_GROUPS_PER_CHUNK_Br_div_64,\n",
        "        'ELEMENTS_PER_GROUP_CONST': ELEMENTS_PER_GROUP_CONST,\n",
        "        'LOG2_L2_BLOCK_SIZE_CONST_KERNEL': LOG2_L2_BLOCK_SIZE_CONST_PYTHON,\n",
        "        'gsize_num_chunks': gsize_num_chunks_val\n",
        "    }\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight_data_actual, quant_state_actual.absmax, quant_state_actual.offset,\n",
        "        quant_state_actual.state2.absmax, quant_state_actual.state2.code, quant_state_actual.code,\n",
        "        output_tensor,\n",
        "        **const_args_for_kernel,\n",
        "        num_warps=num_warps_to_use,\n",
        "        num_stages=num_stages_to_use\n",
        "    )\n",
        "    return output_tensor\n",
        "\n",
        "def your_dequantize_nf4(weight_param):\n",
        "    OPTIMIZED_BR = 4096\n",
        "    OPTIMIZED_WARPS = 4\n",
        "    OPTIMIZED_STAGES = 2\n",
        "    if not hasattr(weight_param, 'weight') or \\\n",
        "       not hasattr(weight_param.weight, 'data') or \\\n",
        "       not hasattr(weight_param.weight, 'quant_state'):\n",
        "        raise ValueError(\"Input 'weight_param' is not the expected Linear4bit layer format or lacks necessary attributes.\")\n",
        "    weight_data_actual = weight_param.weight.data\n",
        "    quant_state_actual = weight_param.weight.quant_state\n",
        "    return _your_dequantize_nf4(\n",
        "        weight_data_actual, quant_state_actual,\n",
        "        CHUNK_SIZE_ELEMENTS_Br=OPTIMIZED_BR,\n",
        "        num_warps_to_use=OPTIMIZED_WARPS,\n",
        "        num_stages_to_use=OPTIMIZED_STAGES\n",
        "    )\n",
        "\n",
        "# --- 2. Corrected Test Harness for torch.compile ---\n",
        "def run_torch_compile_test():\n",
        "    \"\"\"Creates a sample layer, compiles the dequantize function, and runs it.\"\"\"\n",
        "    print(\"--- Setting up test for torch.compile ---\")\n",
        "    try:\n",
        "        # CORRECTED: Using `input_features` and `output_features`\n",
        "        sample_layer = Linear4bit(\n",
        "            input_features=1024,\n",
        "            output_features=2048,\n",
        "            bias=None,\n",
        "            compute_dtype=torch.bfloat16,\n",
        "            quant_type='nf4'\n",
        "        ).to(\"cuda\")\n",
        "        weight_param = sample_layer\n",
        "        print(\"Sample Linear4bit layer created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Failed to create sample layer: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Attempting to JIT compile `your_dequantize_nf4` ---\")\n",
        "    try:\n",
        "        compiled_fn = torch.compile(your_dequantize_nf4, mode=\"max-autotune\")\n",
        "        print(\"torch.compile() call finished without error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[FAILED] torch.compile threw an error during the initial compilation step: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Running the compiled function for the first time (triggers JIT) ---\")\n",
        "    try:\n",
        "        result = compiled_fn(weight_param)\n",
        "        print(\"\\n[SUCCESS] Compiled function ran without errors!\")\n",
        "        print(f\"  Output tensor shape: {result.shape}\")\n",
        "        print(f\"  Output tensor dtype: {result.dtype}\")\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"\\n[FAILED] The compiled function threw an error on its first run:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- 3. Execute the Test ---\n",
        "run_torch_compile_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiMEIwhyA_8A",
        "outputId": "9895b626-873b-4ca0-c22c-2625b8653ac3"
      },
      "id": "CiMEIwhyA_8A",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up test for torch.compile ---\n",
            "Sample Linear4bit layer created successfully.\n",
            "\n",
            "--- Attempting to JIT compile `your_dequantize_nf4` ---\n",
            "torch.compile() call finished without error.\n",
            "\n",
            "--- Running the compiled function for the first time (triggers JIT) ---\n",
            "\n",
            "[SUCCESS] Compiled function ran without errors!\n",
            "  Output tensor shape: torch.Size([2048, 1024])\n",
            "  Output tensor dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VN5rqPo_BnTH"
      },
      "id": "VN5rqPo_BnTH",
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}