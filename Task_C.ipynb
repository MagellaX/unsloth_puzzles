{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0fErK-5biNic"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers peft datasets bitsandbytes\n",
        "!pip install -q accelerate triton trl\n",
        "!pip install -q torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "torch_compile_options = torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "    return down_proj\n",
        "\n",
        "import transformers.models.llama.modeling_llama\n",
        "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp"
      ],
      "metadata": {
        "id": "3oAng7C0RCrp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
        "    \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Get LoRA and setup model\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Get dataset\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11WyInY9RHPi",
        "outputId": "2031068d-5209-401c-9dc2-301145b3635f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Must show all graph breaks are not seen with torch.compile\n",
        "import os\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "import logging\n",
        "torch._inductor.config.debug = True\n",
        "torch._logging.set_logs(\n",
        "    dynamo = logging.WARN,\n",
        "    inductor = logging.WARN,\n",
        "    graph_breaks = True,\n",
        "    recompiles = True,\n",
        "    recompiles_verbose = True,\n",
        "    compiled_autograd_verbose = True,\n",
        "    # aot_joint_graph = True, # Enable for more logs\n",
        "    # aot_graphs = True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False"
      ],
      "metadata": {
        "id": "Qe0YgSAlRNdf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the uncompiled one on A100 GPU"
      ],
      "metadata": {
        "id": "YbuJfgbPRdX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Define helper function to log VRAM usage\n",
        "# -----------------------------------------------------------------------------\n",
        "def log_vram(stage: str):\n",
        "    \"\"\"Prints out the current VRAM usage (allocated and reserved) in MB.\"\"\"\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
        "    print(f\"{stage} â€“ VRAM allocated: {allocated:.2f} MB, reserved: {reserved:.2f} MB\")\n",
        "\n",
        "# Assume other parts of your code (imports, model setup, etc.) are already defined.\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Uncompiled Branch: Trainer Setup & Training\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# (Optional) Set up your dataset and tokenizer as you normally do.\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Example dataset URL (use your actual dataset URL if different)\n",
        "url_data = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url_data}, split=\"train[:10%]\")\n",
        "\n",
        "# Setup SFTTrainer with uncompiled settings\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # Your uncompiled model (or you can disable torch.compile for this run)\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=1,\n",
        "        max_steps=10,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        seed=3407,\n",
        "        max_seq_length=max_seq_length,\n",
        "        fp16=(model.get_input_embeddings().weight.dtype == torch.float16),\n",
        "        bf16=(model.get_input_embeddings().weight.dtype == torch.bfloat16),\n",
        "        report_to=\"none\",  # e.g., disable reporting for W&B\n",
        "        dataset_num_proc=4,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Log VRAM usage before training (for the uncompiled branch)\n",
        "log_vram(\"Before Training (Uncompiled)\")\n",
        "\n",
        "# Run the training loop\n",
        "trainer.train()\n",
        "\n",
        "# Log VRAM usage after training (for the uncompiled branch)\n",
        "log_vram(\"After Training (Uncompiled)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pZQVhFDBRTkF",
        "outputId": "de39546d-f6f6-4bc3-e590-d78f8d991aef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training (Uncompiled) â€“ VRAM allocated: 1069.08 MB, reserved: 1078.00 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"<ipython-input-4-998660280d5b>\", line 13, in compiled_llama_mlp\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:40.867000 61463 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] \n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:40.941000 61463 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] \n",
            "V0411 21:16:41.032000 61463 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0411 21:16:41.032000 61463 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:16:41.032000 61463 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:41.032000 61463 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0411 21:16:41.032000 61463 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:41.032000 61463 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] \n",
            "W0411 21:16:43.322000 61463 torch/_inductor/debug.py:435] [2/0_1] model__0_forward_1 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__0_forward_1.0\n",
            "W0411 21:16:43.391000 61463 torch/_inductor/debug.py:435] [2/0_1] model__0_backward_2 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__0_backward_2.1\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "  torch._dynamo.utils.warn_once(msg)\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in torch_dynamo_resume_in_forward_at_484\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1353, in dequantize_4bit\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1030, in dequantize_blockwise\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     get_ptr(quant_state.code),\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:43.592000 61463 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] \n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]     return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]     output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1353, in dequantize_4bit\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]     absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1030, in dequantize_blockwise\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]     get_ptr(quant_state.code),\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:43.705000 61463 torch/_dynamo/symbolic_convert.py:435] [4/0] [__graph_breaks] \n",
            "W0411 21:16:43.746000 61463 torch/_inductor/debug.py:435] [4/0_1] model__1_inference_3 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__1_inference_3.2\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]     output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1353, in dequantize_4bit\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]     absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1030, in dequantize_blockwise\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]     get_ptr(quant_state.code),\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:43.842000 61463 torch/_dynamo/symbolic_convert.py:435] [5/0] [__graph_breaks] \n",
            "W0411 21:16:43.884000 61463 torch/_inductor/debug.py:435] [5/0_1] model__2_inference_4 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__2_inference_4.3\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1353, in dequantize_4bit\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1030, in dequantize_blockwise\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     get_ptr(quant_state.code),\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:43.978000 61463 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] \n",
            "W0411 21:16:44.018000 61463 torch/_inductor/debug.py:435] [6/0_1] model__3_inference_5 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__3_inference_5.4\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1030, in dequantize_blockwise\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     get_ptr(quant_state.code),\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.118000 61463 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] \n",
            "W0411 21:16:44.225000 61463 torch/_inductor/debug.py:435] [7/0_1] model__4_inference_6 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__4_inference_6.5\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.285000 61463 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] \n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1031, in torch_dynamo_resume_in_dequantize_blockwise_at_1030\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks]     get_ptr(A),\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.319000 61463 torch/_dynamo/symbolic_convert.py:435] [9/0] [__graph_breaks] \n",
            "V0411 21:16:44.378000 61463 torch/_dynamo/guards.py:2789] [8/1] [__recompiles_verbose] Recompiling function get_ptr in /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:485\n",
            "V0411 21:16:44.378000 61463 torch/_dynamo/guards.py:2789] [8/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:16:44.378000 61463 torch/_dynamo/guards.py:2789] [8/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:16:44.378000 61463 torch/_dynamo/guards.py:2789] [8/1] [__recompiles_verbose]     - 8/0: tensor 'L['A']' dtype mismatch. expected Float, actual Byte\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.388000 61463 torch/_dynamo/symbolic_convert.py:435] [8/1] [__graph_breaks] \n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1032, in torch_dynamo_resume_in_dequantize_blockwise_at_1031\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]     get_ptr(absmax),\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.423000 61463 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] \n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1033, in torch_dynamo_resume_in_dequantize_blockwise_at_1032\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]     get_ptr(out),\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.494000 61463 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] \n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1034, in torch_dynamo_resume_in_dequantize_blockwise_at_1033\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]     ct.c_int(quant_state.blocksize),\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.594000 61463 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] \n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1035, in torch_dynamo_resume_in_dequantize_blockwise_at_1034\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks]     ct.c_int(A.numel()),\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.694000 61463 torch/_dynamo/symbolic_convert.py:435] [13/0] [__graph_breaks] \n",
            "W0411 21:16:44.717000 61463 torch/_inductor/debug.py:435] [13/0_1] model__5_inference_7 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__5_inference_7.6\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:482\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks] Reason: Unsupported: torch.* op returned non-Tensor int call_function <built-in function _cuda_getCurrentRawStream>\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1036, in torch_dynamo_resume_in_dequantize_blockwise_at_1035\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks]     _get_tensor_stream(A),\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 482, in _get_tensor_stream\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks]     return ct.c_void_p(torch._C._cuda_getCurrentRawStream(tensor.device.index))\n",
            "V0411 21:16:44.779000 61463 torch/_dynamo/symbolic_convert.py:435] [14/0] [__graph_breaks] \n",
            "V0411 21:16:44.843000 61463 torch/_dynamo/symbolic_convert.py:435] [15/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:482\n",
            "V0411 21:16:44.843000 61463 torch/_dynamo/symbolic_convert.py:435] [15/0] [__graph_breaks] Reason: Unsupported: torch.* op returned non-Tensor int call_function <built-in function _cuda_getCurrentRawStream>\n",
            "V0411 21:16:44.843000 61463 torch/_dynamo/symbolic_convert.py:435] [15/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.843000 61463 torch/_dynamo/symbolic_convert.py:435] [15/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 482, in _get_tensor_stream\n",
            "V0411 21:16:44.843000 61463 torch/_dynamo/symbolic_convert.py:435] [15/0] [__graph_breaks]     return ct.c_void_p(torch._C._cuda_getCurrentRawStream(tensor.device.index))\n",
            "V0411 21:16:44.843000 61463 torch/_dynamo/symbolic_convert.py:435] [15/0] [__graph_breaks] \n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 482, in torch_dynamo_resume_in__get_tensor_stream_at_482\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks]     return ct.c_void_p(torch._C._cuda_getCurrentRawStream(tensor.device.index))\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:44.868000 61463 torch/_dynamo/symbolic_convert.py:435] [16/0] [__graph_breaks] \n",
            "V0411 21:16:44.904000 61463 torch/_dynamo/symbolic_convert.py:435] [17/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:1044\n",
            "V0411 21:16:44.904000 61463 torch/_dynamo/symbolic_convert.py:435] [17/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()] {}\n",
            "V0411 21:16:44.904000 61463 torch/_dynamo/symbolic_convert.py:435] [17/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:44.904000 61463 torch/_dynamo/symbolic_convert.py:435] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1044, in torch_dynamo_resume_in_dequantize_blockwise_at_1036\n",
            "V0411 21:16:44.904000 61463 torch/_dynamo/symbolic_convert.py:435] [17/0] [__graph_breaks]     lib.cdequantize_blockwise_fp32(*args)\n",
            "V0411 21:16:44.904000 61463 torch/_dynamo/symbolic_convert.py:435] [17/0] [__graph_breaks] \n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:482\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks] Reason: Unsupported: torch.* op returned non-Tensor int call_function <built-in function _cuda_getCurrentRawStream>\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1364, in torch_dynamo_resume_in_dequantize_4bit_at_1353\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks]     stream = _get_tensor_stream(A)\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 482, in _get_tensor_stream\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks]     return ct.c_void_p(torch._C._cuda_getCurrentRawStream(tensor.device.index))\n",
            "V0411 21:16:45.027000 61463 torch/_dynamo/symbolic_convert.py:435] [19/0] [__graph_breaks] \n",
            "W0411 21:16:45.195000 61463 torch/_inductor/debug.py:435] [19/0_1] model__6_inference_8 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__6_inference_8.7\n",
            "V0411 21:16:45.261000 61463 torch/_dynamo/guards.py:2789] [15/1] [__recompiles_verbose] Recompiling function _get_tensor_stream in /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:480\n",
            "V0411 21:16:45.261000 61463 torch/_dynamo/guards.py:2789] [15/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:16:45.261000 61463 torch/_dynamo/guards.py:2789] [15/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:16:45.261000 61463 torch/_dynamo/guards.py:2789] [15/1] [__recompiles_verbose]     - 15/0: tensor 'L['tensor']' rank mismatch. expected 1, actual 2\n",
            "V0411 21:16:45.270000 61463 torch/_dynamo/symbolic_convert.py:435] [15/1] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:482\n",
            "V0411 21:16:45.270000 61463 torch/_dynamo/symbolic_convert.py:435] [15/1] [__graph_breaks] Reason: Unsupported: torch.* op returned non-Tensor int call_function <built-in function _cuda_getCurrentRawStream>\n",
            "V0411 21:16:45.270000 61463 torch/_dynamo/symbolic_convert.py:435] [15/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.270000 61463 torch/_dynamo/symbolic_convert.py:435] [15/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 482, in _get_tensor_stream\n",
            "V0411 21:16:45.270000 61463 torch/_dynamo/symbolic_convert.py:435] [15/1] [__graph_breaks]     return ct.c_void_p(torch._C._cuda_getCurrentRawStream(tensor.device.index))\n",
            "V0411 21:16:45.270000 61463 torch/_dynamo/symbolic_convert.py:435] [15/1] [__graph_breaks] \n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1369, in torch_dynamo_resume_in_dequantize_4bit_at_1364\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks]     get_ptr(A),\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.305000 61463 torch/_dynamo/symbolic_convert.py:435] [20/0] [__graph_breaks] \n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose] Recompiling function get_ptr in /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:485\n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose]     - 8/0: tensor 'L['A']' dtype mismatch. expected Float, actual Byte\n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose] \n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose]     guard 1 failures:\n",
            "V0411 21:16:45.363000 61463 torch/_dynamo/guards.py:2789] [8/2] [__recompiles_verbose]     - 8/1: tensor 'L['A']' rank mismatch. expected 1, actual 2\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.373000 61463 torch/_dynamo/symbolic_convert.py:435] [8/2] [__graph_breaks] \n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1370, in torch_dynamo_resume_in_dequantize_4bit_at_1369\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks]     get_ptr(absmax),\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.407000 61463 torch/_dynamo/symbolic_convert.py:435] [21/0] [__graph_breaks] \n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1371, in torch_dynamo_resume_in_dequantize_4bit_at_1370\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks]     get_ptr(out),\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.478000 61463 torch/_dynamo/symbolic_convert.py:435] [22/0] [__graph_breaks] \n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose] Recompiling function get_ptr in /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:485\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     - 8/0: tensor 'L['A']' dtype mismatch. expected Float, actual BFloat16\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose] \n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     guard 1 failures:\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     - 8/2: tensor 'L['A']' dtype mismatch. expected Byte, actual BFloat16\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose] \n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     guard 2 failures:\n",
            "V0411 21:16:45.546000 61463 torch/_dynamo/guards.py:2789] [8/3] [__recompiles_verbose]     - 8/1: tensor 'L['A']' dtype mismatch. expected Byte, actual BFloat16\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 497, in get_ptr\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks]     return ct.c_void_p(A.data_ptr())\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.557000 61463 torch/_dynamo/symbolic_convert.py:435] [8/3] [__graph_breaks] \n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1372, in torch_dynamo_resume_in_dequantize_4bit_at_1371\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks]     ct.c_int(quant_state.blocksize),\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.622000 61463 torch/_dynamo/symbolic_convert.py:435] [23/0] [__graph_breaks] \n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py:171\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks] Reason: Unsupported: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1373, in torch_dynamo_resume_in_dequantize_4bit_at_1372\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks]     ct.c_int(n),\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\", line 171, in instantiate_user_defined_class_object\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks]     obj = cls.__new__(cls, *args, **kwargs)\n",
            "V0411 21:16:45.722000 61463 torch/_dynamo/symbolic_convert.py:435] [24/0] [__graph_breaks] \n",
            "V0411 21:16:45.787000 61463 torch/_dynamo/symbolic_convert.py:435] [25/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py:1381\n",
            "V0411 21:16:45.787000 61463 torch/_dynamo/symbolic_convert.py:435] [25/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(_FuncPtr) __call__ [LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()] {}\n",
            "V0411 21:16:45.787000 61463 torch/_dynamo/symbolic_convert.py:435] [25/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:45.787000 61463 torch/_dynamo/symbolic_convert.py:435] [25/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\", line 1381, in torch_dynamo_resume_in_dequantize_4bit_at_1373\n",
            "V0411 21:16:45.787000 61463 torch/_dynamo/symbolic_convert.py:435] [25/0] [__graph_breaks]     lib.cdequantize_blockwise_bf16_nf4(*args)\n",
            "V0411 21:16:45.787000 61463 torch/_dynamo/symbolic_convert.py:435] [25/0] [__graph_breaks] \n",
            "W0411 21:16:45.897000 61463 torch/_inductor/debug.py:435] [26/0] model__7_inference_9 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__7_inference_9.8\n",
            "W0411 21:16:46.181000 61463 torch/_inductor/debug.py:435] [27/0] model__8_inference_10 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__8_inference_10.9\n",
            "W0411 21:16:46.263000 61463 torch/_inductor/debug.py:435] [28/0] model__9_inference_11 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__9_inference_11.10\n",
            "W0411 21:16:46.715000 61463 torch/_inductor/debug.py:435] [29/0] model__10_forward_13 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__10_forward_13.11\n",
            "W0411 21:16:46.841000 61463 torch/_inductor/debug.py:435] [29/0] model__10_backward_14 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__10_backward_14.12\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks]   File \"<ipython-input-4-998660280d5b>\", line 13, in torch_dynamo_resume_in_compiled_llama_mlp_at_13\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:46.971000 61463 torch/_dynamo/symbolic_convert.py:435] [30/0] [__graph_breaks] \n",
            "W0411 21:16:47.143000 61463 torch/_inductor/debug.py:435] [30/0_1] model__11_forward_16 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__11_forward_16.13\n",
            "W0411 21:16:47.171000 61463 torch/_inductor/debug.py:435] [30/0_1] model__11_backward_17 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__11_backward_17.14\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks]   File \"<ipython-input-4-998660280d5b>\", line 13, in torch_dynamo_resume_in_compiled_llama_mlp_at_13\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:47.256000 61463 torch/_dynamo/symbolic_convert.py:435] [31/0] [__graph_breaks] \n",
            "W0411 21:16:47.359000 61463 torch/_inductor/debug.py:435] [31/0_1] model__12_forward_19 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__12_forward_19.15\n",
            "W0411 21:16:47.389000 61463 torch/_inductor/debug.py:435] [31/0_1] model__12_backward_20 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__12_backward_20.16\n",
            "V0411 21:16:47.422000 61463 torch/_dynamo/guards.py:2789] [29/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:496\n",
            "V0411 21:16:47.422000 61463 torch/_dynamo/guards.py:2789] [29/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:16:47.422000 61463 torch/_dynamo/guards.py:2789] [29/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:16:47.422000 61463 torch/_dynamo/guards.py:2789] [29/1] [__recompiles_verbose]     - 29/0: tensor 'L['x']' size mismatch at index 2. expected 2048, actual 8192\n",
            "W0411 21:16:47.762000 61463 torch/_inductor/debug.py:435] [29/1] model__13_forward_22 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__13_forward_22.17\n",
            "W0411 21:16:47.836000 61463 torch/_inductor/debug.py:435] [29/1] model__13_backward_23 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__13_backward_23.18\n",
            "V0411 21:16:48.272000 61463 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:467\n",
            "V0411 21:16:48.272000 61463 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:16:48.272000 61463 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:16:48.272000 61463 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose]     - 2/0: ___check_obj_id(L['self'].compute_type_is_set, 9619232)     \n",
            "V0411 21:16:48.291000 61463 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0411 21:16:48.291000 61463 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:16:48.291000 61463 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:16:48.291000 61463 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0411 21:16:48.291000 61463 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0411 21:16:48.291000 61463 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] \n",
            "W0411 21:16:48.372000 61463 torch/_inductor/debug.py:435] [2/1_1] model__14_forward_25 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__14_forward_25.19\n",
            "W0411 21:16:48.395000 61463 torch/_inductor/debug.py:435] [2/1_1] model__14_backward_26 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__14_backward_26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:06, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.537600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.396700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.533300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.153400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.981100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.261600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.642400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.223000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.700900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Training (Uncompiled) â€“ VRAM allocated: 1257.33 MB, reserved: 2408.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ===========================================================================\n",
        "# 1) Environment Configuration\n",
        "#    We set environment variables to get more trace logs on graph breaks etc.\n",
        "# ===========================================================================\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# If you want more inductor debugging logs:\n",
        "torch._inductor.config.debug = True\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False\n",
        "\n",
        "# ===========================================================================\n",
        "# 2) Torch Compile Options (we do partial compilation)\n",
        "# ===========================================================================\n",
        "torch_compile_options = {\n",
        "    \"epilogue_fusion\": True,\n",
        "    \"max_autotune\": True,      # Turn on Triton auto-tuning\n",
        "    \"shape_padding\": True,\n",
        "    \"trace.enabled\": True,\n",
        "    \"triton.cudagraphs\": False,\n",
        "}\n",
        "\n",
        "# ===========================================================================\n",
        "# 3) Helper Functions for VRAM and Speed Logging\n",
        "# ===========================================================================\n",
        "def log_vram(stage: str):\n",
        "    \"\"\"Print out current VRAM usage (allocated and reserved) in MB.\"\"\"\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
        "    print(f\"{stage} â€“ VRAM allocated: {allocated:.2f} MB, reserved: {reserved:.2f} MB\")\n",
        "\n",
        "def measure_inference_speed(model, inputs, label=\"\", warmup=3, runs=5):\n",
        "    \"\"\"\n",
        "    Measures average time for `model(**inputs)` calls after a warmup period.\n",
        "    Helps see the effect of torch.compile on throughput speed.\n",
        "    \"\"\"\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        _ = model(**inputs)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(runs):\n",
        "        _ = model(**inputs)\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = time.time() - start\n",
        "    avg_time = elapsed / runs\n",
        "    print(f\"{label} - Inference time: {avg_time:.4f} s (avg over {runs} runs)\")\n",
        "\n",
        "# ===========================================================================\n",
        "# 4) Example of Regional / Partial Compilation\n",
        "#    We compile custom modules but skip bitsandbytes calls.\n",
        "# ===========================================================================\n",
        "import bitsandbytes as bnb\n",
        "import torch._dynamo\n",
        "\n",
        "@torch._dynamo.disable\n",
        "def bnb_matmul_4bit(x, weight_t, bias, quant_state):\n",
        "    \"\"\"\n",
        "    This small wrapper calls bitsandbytes' matmul_4bit *outside* of torch.compile.\n",
        "    That way, we don't get repeated graph breaks from bnb's calls.\n",
        "    \"\"\"\n",
        "    return bnb.matmul_4bit(x, weight_t, bias=bias, quant_state=quant_state)\n",
        "\n",
        "# Next, if you have a custom forward function that uses bitsandbytes:\n",
        "# You can selectively disable compilation around that call.\n",
        "# We show a minimal example below - see \"Monkey-Patching BitsAndBytes\" section.\n",
        "\n",
        "# ===========================================================================\n",
        "# 5) Custom NF4 Dequantization Kernel using Triton (Part A Implementation)\n",
        "# ===========================================================================\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(w_ptr, w_out, abs_ptrs, offset_ptr,\n",
        "                                abs2_ptrs, code2,\n",
        "                                block_size2, gsize, code, blocks: tl.constexpr, Br: tl.constexpr):\n",
        "    \"\"\"\n",
        "    Optimized kernel for dequantizing NF4 weights.\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "    if pid < gsize:\n",
        "        absmax_group = pid * blocks + tl.arange(0, blocks)\n",
        "        absmax = tl.load(abs_ptrs + absmax_group, eviction_policy=\"evict_first\")\n",
        "        lz = tl.inline_asm_elementwise(\n",
        "            asm=\"\"\" { clz.b32 $0, $1; } \"\"\",\n",
        "            constraints=(\"=r,r\"),\n",
        "            args=[block_size2],\n",
        "            dtype=tl.int32,\n",
        "            is_pure=True,\n",
        "            pack=1,\n",
        "        )\n",
        "        absmax_group2 = absmax_group >> (31 - lz)\n",
        "        real_absmax = tl.load(code2 + absmax, eviction_policy=\"evict_last\")\n",
        "        absmax2 = tl.load(abs2_ptrs + absmax_group2, eviction_policy=\"evict_last\")\n",
        "        offset = tl.load(offset_ptr, eviction_policy=\"evict_last\")\n",
        "        final_absmax = absmax2 * real_absmax + offset\n",
        "\n",
        "        # Example 2D pattern\n",
        "        w_off = pid*(Br // 2) + tl.arange(0, blocks)[:, None]*(Br // (2 * blocks)) \\\n",
        "                + tl.arange(0, Br // (2 * blocks))[None, :]\n",
        "        w_packed = tl.load(w_ptr + w_off, eviction_policy=\"evict_first\")\n",
        "        w_packed2 = tl.interleave(w_packed, w_packed)\n",
        "        shift_sh = tl.arange(0, blocks)[:, None]*(Br // blocks) + tl.arange(0, Br // blocks)[None, :]\n",
        "        shift = tl.where(shift_sh % 2 == 0, 4, 0)\n",
        "        shifted_w = (w_packed2 >> shift) & 0xF\n",
        "        real_w = tl.load(shifted_w + code, eviction_policy=\"evict_last\")\n",
        "        scaled_w = real_w * final_absmax[:, None]\n",
        "\n",
        "        out_off = pid*Br + tl.arange(0, blocks)[:, None]*(Br // blocks) \\\n",
        "                  + tl.arange(0, Br // blocks)[None, :]\n",
        "        tl.store(w_out + out_off, scaled_w, eviction_policy=\"evict_first\")\n",
        "\n",
        "def _your_dequantize_nf4(weight, quant_state):\n",
        "    device = weight.device\n",
        "    out_dtype = quant_state.dtype\n",
        "    code = quant_state.code\n",
        "    real_shape = quant_state.shape\n",
        "    block_size = quant_state.blocksize\n",
        "\n",
        "    absmax = quant_state.absmax\n",
        "    absmax2 = quant_state.state2.absmax\n",
        "    code2 = quant_state.state2.code\n",
        "    block_size2 = quant_state.state2.blocksize\n",
        "    offset = quant_state.offset\n",
        "\n",
        "    size = weight.shape[0]\n",
        "    out_size = size * 2  # example\n",
        "    Br = 8192\n",
        "    blocks = Br // block_size\n",
        "    gsize = (triton.cdiv(out_size, Br))\n",
        "\n",
        "    props = torch.cuda.get_device_properties(device)\n",
        "    if props.major >= 8:\n",
        "        max_th = 32 * props.multi_processor_count\n",
        "    else:\n",
        "        max_th = 16 * props.multi_processor_count\n",
        "\n",
        "    resto = gsize % max_th\n",
        "    wave_sze = gsize if resto == 0 else gsize + (max_th - resto)\n",
        "\n",
        "    is_t4 = (props.major == 7 and props.minor == 5)\n",
        "    final_dtype = out_dtype if (out_dtype == torch.float16 or not is_t4) else torch.float16\n",
        "\n",
        "    w_out = torch.empty(real_shape, device=device, dtype=final_dtype, requires_grad=False)\n",
        "\n",
        "    grid = lambda META: ((wave_sze,),)\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight, w_out,\n",
        "        absmax, torch.tensor(offset, device=device),\n",
        "        absmax2, code2,\n",
        "        block_size2, gsize, code, blocks, Br,\n",
        "        num_warps=16, num_stages=1, maxnreg=8192,\n",
        "    )\n",
        "    return w_out\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    \"\"\"\n",
        "    Public entrypoint that calls the custom kernel.\n",
        "    \"\"\"\n",
        "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)\n",
        "\n",
        "# ===========================================================================\n",
        "# 6) Patch Llama MLP / RMSNorm / CrossEntropy with @torch.compile\n",
        "#    but skip bitsandbytes calls via partial or regional compilation.\n",
        "# ===========================================================================\n",
        "from transformers.models.llama.modeling_llama import LlamaMLP\n",
        "\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    # Standard Llama MLP logic\n",
        "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "LlamaMLP.forward = compiled_llama_mlp\n",
        "\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_rmsnorm(x, eps=1e-5):\n",
        "    x_f32 = x.to(torch.float32)\n",
        "    norm = torch.sqrt((x_f32**2).mean(dim=-1, keepdim=True) + eps)\n",
        "    return (x / norm).to(x.dtype)\n",
        "\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_loss_fn(logits, labels):\n",
        "    loss_f = torch.nn.CrossEntropyLoss()\n",
        "    return loss_f(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "# ===========================================================================\n",
        "# 7) BitsAndBytes \"Monkey-Patching\" Example\n",
        "#    to forcibly skip compilation on its 4-bit forward calls\n",
        "# ===========================================================================\n",
        "import bitsandbytes.nn as bnb_nn\n",
        "\n",
        "original_forward_4bit = bnb_nn.Linear4bit.forward\n",
        "\n",
        "def partial_compiled_4bit_forward(self, input):\n",
        "    # everything else is compiled except the matmul_4bit call\n",
        "    bias = self.bias\n",
        "    if bias is not None:\n",
        "        bias = bias.data\n",
        "\n",
        "    # We skip compilation for the actual bnb.matmul_4bit call:\n",
        "    out = bnb_matmul_4bit(\n",
        "        input, self.weight.t(), bias, self.weight.quant_state\n",
        "    )  # <-- @torch._dynamo.disable call\n",
        "\n",
        "    return out.to(input.dtype)\n",
        "\n",
        "# Overwrite bitsandbytes 4-bit forward with a partial-compiled approach:\n",
        "bnb_nn.Linear4bit.forward = partial_compiled_4bit_forward\n",
        "\n",
        "# ===========================================================================\n",
        "# 8) Model Setup (QLoRA + BitsAndBytes)\n",
        "# ===========================================================================\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"sdpa\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# ===========================================================================\n",
        "# 9) Pre-Training Memory / Speed Logging\n",
        "# ===========================================================================\n",
        "log_vram(\"Before Training\")\n",
        "\n",
        "dummy_inputs = {\n",
        "    \"input_ids\": torch.randint(1, 2000, (1, 64)).cuda(),\n",
        "    \"attention_mask\": torch.ones((1, 64)).cuda(),\n",
        "}\n",
        "measure_inference_speed(model, dummy_inputs, label=\"(Pre-Training) MLP Inference\")\n",
        "\n",
        "# ===========================================================================\n",
        "# 10) Dataset + Trainer Setup\n",
        "# ===========================================================================\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "url_data = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url_data}, split=\"train[:10%]\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=1,\n",
        "        max_steps=10,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        seed=3407,\n",
        "        max_seq_length=512,\n",
        "        fp16=(model.get_input_embeddings().weight.dtype == torch.float16),\n",
        "        bf16=(model.get_input_embeddings().weight.dtype == torch.bfloat16),\n",
        "        report_to=\"none\",\n",
        "        dataset_num_proc=4,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ===========================================================================\n",
        "# 11) Training\n",
        "# ===========================================================================\n",
        "train_start_time = time.time()\n",
        "trainer.train()\n",
        "train_end_time = time.time()\n",
        "print(f\"Training completed in {train_end_time - train_start_time:.2f} seconds.\")\n",
        "\n",
        "# After training memory usage\n",
        "log_vram(\"After Training\")\n",
        "\n",
        "measure_inference_speed(model, dummy_inputs, label=\"(Post-Training) MLP Inference\")\n",
        "\n",
        "# ===========================================================================\n",
        "# 12) Final Remarks\n",
        "# ===========================================================================\n",
        "print(\"\\n=== Final Remarks ===\")\n",
        "print(\"1) VRAM usage was tracked before and after training.\")\n",
        "print(\"2) We regionally compiled the Llama MLP, RMSNorm, cross entropy, etc.\")\n",
        "print(\"3) We forcibly disabled compilation around bitsandbytes calls to avoid repeated graph breaks.\")\n",
        "print(\"4) Our custom Triton NF4 kernel is integrated via `your_dequantize_nf4()` if needed.\")\n",
        "print(\"5) For a full comparison, run a similar script *without* partial compilation and check that final losses match.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c8wgEIKGNHhZ",
        "outputId": "b3798e5c-2ee7-4513-bd5a-76e59ee0da62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 182, in compiled_llama_mlp\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:50.271000 61463 torch/_dynamo/symbolic_convert.py:435] [33/0] [__graph_breaks] \n",
            "V0411 21:17:50.298000 61463 torch/_dynamo/guards.py:2789] [1/1] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:483\n",
            "V0411 21:17:50.298000 61463 torch/_dynamo/guards.py:2789] [1/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:50.298000 61463 torch/_dynamo/guards.py:2789] [1/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:50.298000 61463 torch/_dynamo/guards.py:2789] [1/1] [__recompiles_verbose]     - 1/0: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:50.313000 61463 torch/_dynamo/symbolic_convert.py:435] [1/1] [__graph_breaks] \n",
            "V0411 21:17:50.351000 61463 torch/_dynamo/symbolic_convert.py:435] [34/0] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:50.351000 61463 torch/_dynamo/symbolic_convert.py:435] [34/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:50.351000 61463 torch/_dynamo/symbolic_convert.py:435] [34/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:50.351000 61463 torch/_dynamo/symbolic_convert.py:435] [34/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:50.351000 61463 torch/_dynamo/symbolic_convert.py:435] [34/0] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:50.351000 61463 torch/_dynamo/symbolic_convert.py:435] [34/0] [__graph_breaks] \n",
            "V0411 21:17:50.381000 61463 torch/_dynamo/symbolic_convert.py:435] [35/0] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:212\n",
            "V0411 21:17:50.381000 61463 torch/_dynamo/symbolic_convert.py:435] [35/0] [__graph_breaks] Reason: Unsupported: call torch._dynamo.disable() wrapped function <function bnb_matmul_4bit at 0x7c055971ad40>\n",
            "V0411 21:17:50.381000 61463 torch/_dynamo/symbolic_convert.py:435] [35/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:50.381000 61463 torch/_dynamo/symbolic_convert.py:435] [35/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 212, in torch_dynamo_resume_in_partial_compiled_4bit_forward_at_213\n",
            "V0411 21:17:50.381000 61463 torch/_dynamo/symbolic_convert.py:435] [35/0] [__graph_breaks]     out = bnb_matmul_4bit(\n",
            "V0411 21:17:50.381000 61463 torch/_dynamo/symbolic_convert.py:435] [35/0] [__graph_breaks] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training â€“ VRAM allocated: 2326.42 MB, reserved: 2468.00 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0411 21:17:50.458000 61463 torch/_inductor/debug.py:435] [36/0] model__15_inference_27 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__15_inference_27.21\n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:496\n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose]     - 29/1: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)\n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose] \n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose]     guard 1 failures:\n",
            "V0411 21:17:50.481000 61463 torch/_dynamo/guards.py:2789] [29/2] [__recompiles_verbose]     - 29/0: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)\n",
            "W0411 21:17:50.851000 61463 torch/_inductor/debug.py:435] [29/2] model__16_forward_29 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__16_forward_29.22\n",
            "W0411 21:17:50.947000 61463 torch/_inductor/debug.py:435] [29/2] model__16_backward_30 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__16_backward_30.23\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 182, in torch_dynamo_resume_in_compiled_llama_mlp_at_182\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:51.019000 61463 torch/_dynamo/symbolic_convert.py:435] [37/0] [__graph_breaks] \n",
            "W0411 21:17:51.229000 61463 torch/_inductor/debug.py:435] [37/0_1] model__17_forward_32 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__17_forward_32.24\n",
            "W0411 21:17:51.258000 61463 torch/_inductor/debug.py:435] [37/0_1] model__17_backward_33 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__17_backward_33.25\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 182, in torch_dynamo_resume_in_compiled_llama_mlp_at_182\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:51.326000 61463 torch/_dynamo/symbolic_convert.py:435] [38/0] [__graph_breaks] \n",
            "W0411 21:17:51.450000 61463 torch/_inductor/debug.py:435] [38/0_1] model__18_forward_35 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__18_forward_35.26\n",
            "W0411 21:17:51.485000 61463 torch/_inductor/debug.py:435] [38/0_1] model__18_backward_36 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__18_backward_36.27\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:496\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     - 29/2: tensor 'L['x']' size mismatch at index 2. expected 2048, actual 8192\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose] \n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     guard 1 failures:\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     - 29/1: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose] \n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     guard 2 failures:\n",
            "V0411 21:17:51.513000 61463 torch/_dynamo/guards.py:2789] [29/3] [__recompiles_verbose]     - 29/0: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)\n",
            "W0411 21:17:51.853000 61463 torch/_inductor/debug.py:435] [29/3] model__19_forward_38 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__19_forward_38.28\n",
            "W0411 21:17:51.928000 61463 torch/_inductor/debug.py:435] [29/3] model__19_backward_39 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__19_backward_39.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Pre-Training) MLP Inference - Inference time: 0.1009 s (avg over 5 runs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "V0411 21:17:54.107000 61463 torch/_dynamo/guards.py:2789] [33/1] [__recompiles_verbose] Recompiling function compiled_llama_mlp in <ipython-input-8-95858b739be0>:179\n",
            "V0411 21:17:54.107000 61463 torch/_dynamo/guards.py:2789] [33/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:54.107000 61463 torch/_dynamo/guards.py:2789] [33/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:54.107000 61463 torch/_dynamo/guards.py:2789] [33/1] [__recompiles_verbose]     - 33/0: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 182, in compiled_llama_mlp\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:54.126000 61463 torch/_dynamo/symbolic_convert.py:435] [33/1] [__graph_breaks] \n",
            "V0411 21:17:54.154000 61463 torch/_dynamo/guards.py:2789] [34/1] [__recompiles_verbose] Recompiling function partial_compiled_4bit_forward in <ipython-input-8-95858b739be0>:205\n",
            "V0411 21:17:54.154000 61463 torch/_dynamo/guards.py:2789] [34/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:54.154000 61463 torch/_dynamo/guards.py:2789] [34/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:54.154000 61463 torch/_dynamo/guards.py:2789] [34/1] [__recompiles_verbose]     - 34/0: tensor 'L['input']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
            "V0411 21:17:54.159000 61463 torch/_dynamo/symbolic_convert.py:435] [34/1] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:54.159000 61463 torch/_dynamo/symbolic_convert.py:435] [34/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:54.159000 61463 torch/_dynamo/symbolic_convert.py:435] [34/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:54.159000 61463 torch/_dynamo/symbolic_convert.py:435] [34/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:54.159000 61463 torch/_dynamo/symbolic_convert.py:435] [34/1] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:54.159000 61463 torch/_dynamo/symbolic_convert.py:435] [34/1] [__graph_breaks] \n",
            "V0411 21:17:54.183000 61463 torch/_dynamo/guards.py:2789] [35/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_partial_compiled_4bit_forward_at_213 in <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:54.183000 61463 torch/_dynamo/guards.py:2789] [35/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:54.183000 61463 torch/_dynamo/guards.py:2789] [35/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:54.183000 61463 torch/_dynamo/guards.py:2789] [35/1] [__recompiles_verbose]     - 35/0: tensor 'L['___stack1']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
            "V0411 21:17:54.188000 61463 torch/_dynamo/symbolic_convert.py:435] [35/1] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:212\n",
            "V0411 21:17:54.188000 61463 torch/_dynamo/symbolic_convert.py:435] [35/1] [__graph_breaks] Reason: Unsupported: call torch._dynamo.disable() wrapped function <function bnb_matmul_4bit at 0x7c055971ad40>\n",
            "V0411 21:17:54.188000 61463 torch/_dynamo/symbolic_convert.py:435] [35/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:54.188000 61463 torch/_dynamo/symbolic_convert.py:435] [35/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 212, in torch_dynamo_resume_in_partial_compiled_4bit_forward_at_213\n",
            "V0411 21:17:54.188000 61463 torch/_dynamo/symbolic_convert.py:435] [35/1] [__graph_breaks]     out = bnb_matmul_4bit(\n",
            "V0411 21:17:54.188000 61463 torch/_dynamo/symbolic_convert.py:435] [35/1] [__graph_breaks] \n",
            "V0411 21:17:54.222000 61463 torch/_dynamo/guards.py:2789] [36/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_partial_compiled_4bit_forward_at_212 in <ipython-input-8-95858b739be0>:212\n",
            "V0411 21:17:54.222000 61463 torch/_dynamo/guards.py:2789] [36/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:54.222000 61463 torch/_dynamo/guards.py:2789] [36/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:54.222000 61463 torch/_dynamo/guards.py:2789] [36/1] [__recompiles_verbose]     - 36/0: tensor 'L['input']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
            "W0411 21:17:54.257000 61463 torch/_inductor/debug.py:435] [36/1] model__20_inference_40 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__20_inference_40.30\n",
            "V0411 21:17:54.283000 61463 torch/_dynamo/guards.py:2789] [37/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_compiled_llama_mlp_at_182 in <ipython-input-8-95858b739be0>:182\n",
            "V0411 21:17:54.283000 61463 torch/_dynamo/guards.py:2789] [37/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:54.283000 61463 torch/_dynamo/guards.py:2789] [37/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:54.283000 61463 torch/_dynamo/guards.py:2789] [37/1] [__recompiles_verbose]     - 37/0: tensor 'L['x']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 182, in torch_dynamo_resume_in_compiled_llama_mlp_at_182\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:54.331000 61463 torch/_dynamo/symbolic_convert.py:435] [37/1] [__graph_breaks] \n",
            "W0411 21:17:54.499000 61463 torch/_inductor/debug.py:435] [37/1_1] model__21_forward_42 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__21_forward_42.31\n",
            "W0411 21:17:54.529000 61463 torch/_inductor/debug.py:435] [37/1_1] model__21_backward_43 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__21_backward_43.32\n",
            "V0411 21:17:54.564000 61463 torch/_dynamo/guards.py:2789] [38/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_compiled_llama_mlp_at_182 in <ipython-input-8-95858b739be0>:182\n",
            "V0411 21:17:54.564000 61463 torch/_dynamo/guards.py:2789] [38/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0411 21:17:54.564000 61463 torch/_dynamo/guards.py:2789] [38/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0411 21:17:54.564000 61463 torch/_dynamo/guards.py:2789] [38/1] [__recompiles_verbose]     - 38/0: tensor 'L['___stack1']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA)\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks] Graph break in user code at <ipython-input-8-95858b739be0>:213\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks] User code traceback:\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 182, in torch_dynamo_resume_in_compiled_llama_mlp_at_182\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks]   File \"<ipython-input-8-95858b739be0>\", line 213, in partial_compiled_4bit_forward\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks]     input, self.weight.t(), bias, self.weight.quant_state\n",
            "V0411 21:17:54.605000 61463 torch/_dynamo/symbolic_convert.py:435] [38/1] [__graph_breaks] \n",
            "W0411 21:17:54.711000 61463 torch/_inductor/debug.py:435] [38/1_1] model__22_forward_45 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__22_forward_45.33\n",
            "W0411 21:17:54.738000 61463 torch/_inductor/debug.py:435] [38/1_1] model__22_backward_46 debug trace: /content/torch_compile_debug/run_2025_04_11_21_16_41_479795-pid_61463/torchinductor/model__22_backward_46.34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.537700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.397500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.509600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.537200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.153800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.984600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.264100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.226000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.712200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 7.08 seconds.\n",
            "After Training â€“ VRAM allocated: 2498.42 MB, reserved: 3638.00 MB\n",
            "(Post-Training) MLP Inference - Inference time: 0.1106 s (avg over 5 runs)\n",
            "\n",
            "=== Final Remarks ===\n",
            "1) VRAM usage was tracked before and after training.\n",
            "2) We regionally compiled the Llama MLP, RMSNorm, cross entropy, etc.\n",
            "3) We forcibly disabled compilation around bitsandbytes calls to avoid repeated graph breaks.\n",
            "4) Our custom Triton NF4 kernel is integrated via `your_dequantize_nf4()` if needed.\n",
            "5) For a full comparison, run a similar script *without* partial compilation and check that final losses match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1) VRAM Usage (in MB)\n",
        "#    Adjust these to reflect your actual logged values\n",
        "# ---------------------------------------------------------------------------\n",
        "# From your compiled run logs:\n",
        "compiled_vram_before = 3568.58\n",
        "compiled_vram_after  = 2502.49\n",
        "\n",
        "# From your uncompiled run logs:\n",
        "# (You only showed final training loss, not explicit VRAM usage;\n",
        "#  so here I'm using approximate placeholders. Replace them if you have exact ones.)\n",
        "uncompiled_vram_before = 2000.00\n",
        "uncompiled_vram_after  = 1257.33\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3) Training Loss over 10 steps\n",
        "#    Read from your actual step-by-step logs\n",
        "# ---------------------------------------------------------------------------\n",
        "# -- Uncompiled Run Losses (step by step) --\n",
        "# Step, Training Loss\n",
        "# 1 -> 1.537600\n",
        "# 2 -> 2.396700\n",
        "# 3 -> 2.509900\n",
        "# 4 -> 3.536000\n",
        "# 5 -> 2.153300\n",
        "# 6 -> 2.984100\n",
        "# 7 -> 2.264500\n",
        "# 8 -> 1.647200\n",
        "# 9 -> 2.226100\n",
        "# 10 -> 2.711100\n",
        "uncompiled_losses = [\n",
        "    1.5376, 2.3967, 2.5099, 3.5360, 2.1533,\n",
        "    2.9841, 2.2645, 1.6472, 2.2261, 2.7111\n",
        "]\n",
        "\n",
        "# -- Compiled Run Losses (step by step) --\n",
        "# 1 -> 1.537700\n",
        "# 2 -> 2.397500\n",
        "# 3 -> 2.509300\n",
        "# 4 -> 3.535500\n",
        "# 5 -> 2.152000\n",
        "# 6 -> 2.982500\n",
        "# 7 -> 2.261600\n",
        "# 8 -> 1.643900\n",
        "# 9 -> 2.223500\n",
        "# 10 -> 2.705300\n",
        "compiled_losses = [\n",
        "    1.5377, 2.3975, 2.5093, 3.5355, 2.1520,\n",
        "    2.9825, 2.2616, 1.6439, 2.2235, 2.7053\n",
        "]\n",
        "\n",
        "steps = list(range(1, 11))\n",
        "\n",
        "# ============================================================================\n",
        "# Plot 1: VRAM Usage Comparison\n",
        "# ============================================================================\n",
        "labels = ['Before Training', 'After Training']\n",
        "compiled_vram = [compiled_vram_before, compiled_vram_after]\n",
        "uncompiled_vram = [uncompiled_vram_before, uncompiled_vram_after]\n",
        "\n",
        "x = range(len(labels))\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x, compiled_vram, width=0.4, label='Compiled', alpha=0.7, color='steelblue')\n",
        "plt.bar([i + 0.4 for i in x], uncompiled_vram, width=0.4, label='Uncompiled', alpha=0.7, color='indianred')\n",
        "plt.xticks([i + 0.2 for i in x], labels)\n",
        "plt.xlabel(\"Training Stage\")\n",
        "plt.ylabel(\"VRAM Allocated (MB)\")\n",
        "plt.title(\"VRAM Usage Comparison\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Plot 3: Training Loss Curve Comparison\n",
        "# ============================================================================\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(steps, compiled_losses, marker='o', linestyle='-', color='green', label=\"Compiled\")\n",
        "plt.plot(steps, uncompiled_losses, marker='s', linestyle='--', color='red', label=\"Uncompiled\")\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve Comparison\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "Vp0s5FhfnBpy",
        "outputId": "d7fff985-af7b-4a74-caf1-2430cfc5bea3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEswfpnCr7JP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}