{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0fErK-5biNic"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers peft datasets bitsandbytes\n",
        "!pip install -q accelerate triton trl\n",
        "!pip install -q torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "torch_compile_options = torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "    return down_proj\n",
        "\n",
        "import transformers.models.llama.modeling_llama\n",
        "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp"
      ],
      "metadata": {
        "id": "3oAng7C0RCrp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
        "    \"expandable_segments:True,\"\\\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Get LoRA and setup model\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
        "        else: param.requires_grad_(False)\n",
        "\n",
        "# Currently GC will cause torch.compile to be disabled, so disable it\n",
        "# model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Get dataset\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11WyInY9RHPi",
        "outputId": "0a5cf88f-01f6-44cf-b1e8-5c9483aa7f3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Must show all graph breaks are not seen with torch.compile\n",
        "import os\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "import logging\n",
        "torch._inductor.config.debug = True\n",
        "torch._logging.set_logs(\n",
        "    dynamo = logging.WARN,\n",
        "    inductor = logging.WARN,\n",
        "    graph_breaks = True,\n",
        "    recompiles = True,\n",
        "    recompiles_verbose = True,\n",
        "    compiled_autograd_verbose = True,\n",
        "    # aot_joint_graph = True, # Enable for more logs\n",
        "    # aot_graphs = True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False"
      ],
      "metadata": {
        "id": "Qe0YgSAlRNdf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the uncompiled one on A100 GPU"
      ],
      "metadata": {
        "id": "YbuJfgbPRdX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Define helper function to log VRAM usage\n",
        "# -----------------------------------------------------------------------------\n",
        "def log_vram(stage: str):\n",
        "    \"\"\"Prints out the current VRAM usage (allocated and reserved) in MB.\"\"\"\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
        "    print(f\"{stage} – VRAM allocated: {allocated:.2f} MB, reserved: {reserved:.2f} MB\")\n",
        "\n",
        "# Assume other parts of your code (imports, model setup, etc.) are already defined.\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Uncompiled Branch: Trainer Setup & Training\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# (Optional) Set up your dataset and tokenizer as you normally do.\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Example dataset URL (use your actual dataset URL if different)\n",
        "url_data = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url_data}, split=\"train[:10%]\")\n",
        "\n",
        "# Setup SFTTrainer with uncompiled settings\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # Your uncompiled model (or you can disable torch.compile for this run)\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=1,\n",
        "        max_steps=10,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        seed=3407,\n",
        "        max_seq_length=max_seq_length,\n",
        "        fp16=(model.get_input_embeddings().weight.dtype == torch.float16),\n",
        "        bf16=(model.get_input_embeddings().weight.dtype == torch.bfloat16),\n",
        "        report_to=\"none\",  # e.g., disable reporting for W&B\n",
        "        dataset_num_proc=4,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Log VRAM usage before training (for the uncompiled branch)\n",
        "log_vram(\"Before Training (Uncompiled)\")\n",
        "\n",
        "# Run the training loop\n",
        "trainer.train()\n",
        "\n",
        "# Log VRAM usage after training (for the uncompiled branch)\n",
        "log_vram(\"After Training (Uncompiled)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pZQVhFDBRTkF",
        "outputId": "0db26df2-fc7e-4d95-fa87-3c2db6e32709"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training (Uncompiled) – VRAM allocated: 1066.08 MB, reserved: 1548.00 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"<ipython-input-2-3830617507>\", line 13, in compiled_llama_mlp\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 494, in forward\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 496, in forward\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0611 16:20:25.328000 39833 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] \n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 494, in forward\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 496, in forward\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0611 16:20:25.400000 39833 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] \n",
            "V0611 16:20:25.483000 39833 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:25.483000 39833 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:25.483000 39833 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:25.483000 39833 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 496, in forward\n",
            "V0611 16:20:25.483000 39833 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0611 16:20:25.483000 39833 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] \n",
            "W0611 16:20:27.815000 39833 torch/_inductor/debug.py:435] [2/0_1] model__0_forward_1 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__0_forward_1.0\n",
            "W0611 16:20:27.886000 39833 torch/_inductor/debug.py:435] [2/0_1] model__0_backward_2 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__0_backward_2.1\n",
            "V0611 16:20:27.968000 39833 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:298\n",
            "V0611 16:20:27.968000 39833 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:27.968000 39833 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:27.968000 39833 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 298, in __torch_function__\n",
            "V0611 16:20:27.968000 39833 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks]     return func(*args, **kwargs)\n",
            "V0611 16:20:27.968000 39833 torch/_dynamo/symbolic_convert.py:435] [3/0] [__graph_breaks] \n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] Graph break: skip: from user code at:\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 298, in __torch_function__\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return func(*args, **kwargs)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] Traceback (most recent call last):\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/codegen.py\", line 263, in __call__\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self.call_reconstruct(value)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/codegen.py\", line 90, in call_reconstruct\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     res = value.reconstruct(self)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/ctx_manager.py\", line 1228, in reconstruct\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self.ctx.reconstruct_type(codegen)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/ctx_manager.py\", line 94, in reconstruct_type\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     AttrSource(codegen.tx.import_source(self.module_name()), self.fn_name())\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]                                         ^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/ctx_manager.py\", line 106, in module_name\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     raise NotImplementedError(\"module_name called on base\")\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] NotImplementedError: module_name called on base\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] \n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] During handling of the above exception, another exception occurred:\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] \n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] Traceback (most recent call last):\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     result = self._inner_convert(\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]              ^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return _compile(\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]            ^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return function(*args, **kwargs)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     out_code = transform_code_object(code, transform)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     transformations(instructions, code_options)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return fn(*args, **kwargs)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     tracer.run()\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     super().run()\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     while self.step():\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]           ^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self.dispatch_table[inst.opcode](self, inst)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 657, in wrapper\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return handle_graph_break(self, inst, speculation.reason)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 698, in handle_graph_break\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self.output.compile_subgraph(self, reason=reason)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1120, in compile_subgraph\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self.codegen_suffix(tx, stack_values, pass1)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1193, in codegen_suffix\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     cg.restore_stack(stack_values, value_from_source=not tx.export)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/codegen.py\", line 82, in restore_stack\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self.foreach(stack_values)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/codegen.py\", line 293, in foreach\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     self(i)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/codegen.py\", line 265, in __call__\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     unimplemented(f\"reconstruct: {value}\")\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\", line 317, in unimplemented\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     raise Unsupported(msg, case_name=case_name)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] torch._dynamo.exc.Unsupported: reconstruct: WithExitFunctionVariable()\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] \n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] from user code:\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]    File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 298, in __torch_function__\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     return func(*args, **kwargs)\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] \n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] \n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] You can suppress this exception and fall back to eager by setting:\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     import torch._dynamo\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks]     torch._dynamo.config.suppress_errors = True\n",
            "V0611 16:20:27.979000 39833 torch/_dynamo/convert_frame.py:1214] [3/0] [__graph_breaks] \n",
            "W0611 16:20:28.487000 39833 torch/_inductor/debug.py:435] [4/0] model__1_forward_4 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__1_forward_4.2\n",
            "W0611 16:20:28.565000 39833 torch/_inductor/debug.py:435] [4/0] model__1_backward_5 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__1_backward_5.3\n",
            "W0611 16:20:29.031000 39833 torch/_inductor/debug.py:435] [5/0] model__2_forward_7 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__2_forward_7.4\n",
            "W0611 16:20:29.114000 39833 torch/_inductor/debug.py:435] [5/0] model__2_backward_8 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__2_backward_8.5\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"<ipython-input-2-3830617507>\", line 13, in torch_dynamo_resume_in_compiled_llama_mlp_at_13\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 494, in forward\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 496, in forward\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0611 16:20:29.231000 39833 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] \n",
            "W0611 16:20:29.404000 39833 torch/_inductor/debug.py:435] [6/0_1] model__3_forward_10 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__3_forward_10.6\n",
            "W0611 16:20:29.432000 39833 torch/_inductor/debug.py:435] [6/0_1] model__3_backward_11 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__3_backward_11.7\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"<ipython-input-2-3830617507>\", line 13, in torch_dynamo_resume_in_compiled_llama_mlp_at_13\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 494, in forward\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 496, in forward\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0611 16:20:29.521000 39833 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] \n",
            "W0611 16:20:29.615000 39833 torch/_inductor/debug.py:435] [7/0_1] model__4_forward_13 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__4_forward_13.8\n",
            "W0611 16:20:29.644000 39833 torch/_inductor/debug.py:435] [7/0_1] model__4_backward_14 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__4_backward_14.9\n",
            "V0611 16:20:29.677000 39833 torch/_dynamo/guards.py:2789] [4/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:29.677000 39833 torch/_dynamo/guards.py:2789] [4/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0611 16:20:29.677000 39833 torch/_dynamo/guards.py:2789] [4/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0611 16:20:29.677000 39833 torch/_dynamo/guards.py:2789] [4/1] [__recompiles_verbose]     - 4/0: tensor 'L['___stack1']' size mismatch at index 2. expected 2048, actual 8192\n",
            "W0611 16:20:30.004000 39833 torch/_inductor/debug.py:435] [4/1] model__5_forward_16 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__5_forward_16.10\n",
            "W0611 16:20:30.030000 39833 torch/_inductor/debug.py:435] [4/1] model__5_backward_17 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__5_backward_17.11\n",
            "V0611 16:20:30.078000 39833 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_494 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:494\n",
            "V0611 16:20:30.078000 39833 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0611 16:20:30.078000 39833 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0611 16:20:30.078000 39833 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose]     - 5/0: tensor 'L['x']' size mismatch at index 2. expected 2048, actual 8192\n",
            "W0611 16:20:30.433000 39833 torch/_inductor/debug.py:435] [5/1] model__6_forward_19 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__6_forward_19.12\n",
            "W0611 16:20:30.472000 39833 torch/_inductor/debug.py:435] [5/1] model__6_backward_20 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__6_backward_20.13\n",
            "V0611 16:20:30.875000 39833 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose] Recompiling function forward in /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:479\n",
            "V0611 16:20:30.875000 39833 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0611 16:20:30.875000 39833 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0611 16:20:30.875000 39833 torch/_dynamo/guards.py:2789] [2/1] [__recompiles_verbose]     - 2/0: ___check_obj_id(L['self'].compute_type_is_set, 9623328)     \n",
            "V0611 16:20:30.897000 39833 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:496\n",
            "V0611 16:20:30.897000 39833 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0611 16:20:30.897000 39833 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] User code traceback:\n",
            "V0611 16:20:30.897000 39833 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 496, in forward\n",
            "V0611 16:20:30.897000 39833 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0611 16:20:30.897000 39833 torch/_dynamo/symbolic_convert.py:435] [2/1] [__graph_breaks] \n",
            "W0611 16:20:30.984000 39833 torch/_inductor/debug.py:435] [2/1_1] model__7_forward_22 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__7_forward_22.14\n",
            "W0611 16:20:31.007000 39833 torch/_inductor/debug.py:435] [2/1_1] model__7_backward_23 debug trace: /content/torch_compile_debug/run_2025_06_11_16_20_25_926657-pid_39833/torchinductor/model__7_backward_23.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:06, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.538400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.397200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.509300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.534300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.152400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.982100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.262600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.221500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.703800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Training (Uncompiled) – VRAM allocated: 1254.33 MB, reserved: 4004.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# 1. Imports\n",
        "# ===========================================================================\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import math\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "# *** CORRECTED IMPORTS FOR PATCHING ***\n",
        "from transformers.models.llama.modeling_llama import (\n",
        "    LlamaForCausalLM,\n",
        "    LlamaMLP,\n",
        "    LlamaRMSNorm,\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import load_dataset\n",
        "import bitsandbytes.nn as bnb_nn\n",
        "from bitsandbytes.nn.modules import Linear4bit\n",
        "\n",
        "# ===========================================================================\n",
        "# 2. Environment & Logging Configuration\n",
        "# Set environment variables for detailed graph break and re-compilation logs.\n",
        "# ===========================================================================\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "# Set to 0 to disable recompilation logs if they become too noisy\n",
        "os.environ[\"TORCHDYNAMO_REPRO_AFTER\"] = \"0\"\n",
        "torch._logging.set_logs(\n",
        "    dynamo=logging.INFO,\n",
        "    aot=logging.INFO,\n",
        "    inductor=logging.INFO,\n",
        "    graph_breaks=True,\n",
        "    recompiles=True,\n",
        "    recompiles_verbose=True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False\n",
        "\n",
        "# ===========================================================================\n",
        "# 3. Task A: Triton Kernel for NF4 Dequantization\n",
        "# This kernel and its launcher are from your puzzle_1.ipynb solution.\n",
        "# It is torch.compile-compatible and will replace the bnb forward pass.\n",
        "# ===========================================================================\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(\n",
        "    w_ptr, abs_idx_ptr, offset_ptr, abs2_scales_ptr, code2_ptr, nf4_code_ptr, output_ptr,\n",
        "    TOTAL_ELEMENTS_IN_OUTPUT_TENSOR: tl.constexpr,\n",
        "    BLOCK_SIZE_BYTES_PER_CHUNK: tl.constexpr,\n",
        "    BLOCK_SIZE_ELEMENTS_PER_CHUNK: tl.constexpr,\n",
        "    NUM_GROUPS_PER_CHUNK: tl.constexpr,\n",
        "    ELEMENTS_PER_GROUP_CONST: tl.constexpr,\n",
        "    LOG2_L2_BLOCK_SIZE_CONST_KERNEL: tl.constexpr,\n",
        "    gsize_num_chunks: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    if pid >= gsize_num_chunks: return\n",
        "\n",
        "    log2_l2_block_size = LOG2_L2_BLOCK_SIZE_CONST_KERNEL\n",
        "    chunk_element_start_offset = pid * BLOCK_SIZE_ELEMENTS_PER_CHUNK\n",
        "    group_arange_local = tl.arange(0, NUM_GROUPS_PER_CHUNK)\n",
        "    absmax_group_indices_potential = (chunk_element_start_offset // ELEMENTS_PER_GROUP_CONST) + group_arange_local\n",
        "    group_mask = (absmax_group_indices_potential * ELEMENTS_PER_GROUP_CONST) < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "\n",
        "    quantized_absmax_indices = tl.load(abs_idx_ptr + absmax_group_indices_potential, mask=group_mask, other=0, eviction_policy=\"evict_first\")\n",
        "    dequantized_l1_scales = tl.load(code2_ptr + quantized_absmax_indices.to(tl.int32), mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "\n",
        "    absmax_l2_group_indices_potential = absmax_group_indices_potential >> log2_l2_block_size\n",
        "    l2_scales = tl.load(abs2_scales_ptr + absmax_l2_group_indices_potential, mask=group_mask, other=0.0, eviction_policy=\"evict_last\")\n",
        "\n",
        "    offset_val = tl.load(offset_ptr + 0)\n",
        "    final_group_scales_masked = l2_scales * dequantized_l1_scales + offset_val\n",
        "\n",
        "    element_arange_pid_local = tl.arange(0, BLOCK_SIZE_ELEMENTS_PER_CHUNK)\n",
        "    global_element_indices = chunk_element_start_offset + element_arange_pid_local\n",
        "    element_op_mask = global_element_indices < TOTAL_ELEMENTS_IN_OUTPUT_TENSOR\n",
        "\n",
        "    byte_index_global_for_element = global_element_indices // 2\n",
        "    is_low_nibble_flag = (global_element_indices % 2) != 0\n",
        "\n",
        "    packed_byte_for_element = tl.load(w_ptr + byte_index_global_for_element, mask=element_op_mask, other=0)\n",
        "    nibble_shift = tl.where(is_low_nibble_flag, 0, 4)\n",
        "    quantized_idx_for_element = (packed_byte_for_element >> nibble_shift) & 0x0F\n",
        "\n",
        "    dequant_val_for_element = tl.load(nf4_code_ptr + quantized_idx_for_element.to(tl.int32), mask=element_op_mask, other=0.0)\n",
        "\n",
        "    scales_reshaped_for_broadcast = tl.reshape(final_group_scales_masked, (NUM_GROUPS_PER_CHUNK, 1))\n",
        "    scales_broadcasted_to_elements = tl.broadcast_to(scales_reshaped_for_broadcast, (NUM_GROUPS_PER_CHUNK, ELEMENTS_PER_GROUP_CONST))\n",
        "    element_scales_vector = tl.reshape(scales_broadcasted_to_elements, (BLOCK_SIZE_ELEMENTS_PER_CHUNK,))\n",
        "    final_scale_for_element = tl.where(element_op_mask, element_scales_vector, 0.0)\n",
        "\n",
        "    scaled_element_output = dequant_val_for_element * final_scale_for_element\n",
        "    tl.store(output_ptr + global_element_indices, scaled_element_output, mask=element_op_mask)\n",
        "\n",
        "def _your_dequantize_nf4(weight_data, quant_state):\n",
        "    device = weight_data.device\n",
        "    output_shape = quant_state.shape\n",
        "    total_elements_in_output = output_shape.numel()\n",
        "\n",
        "    # Kernel launch parameters\n",
        "    OPTIMIZED_BR = 4096\n",
        "    OPTIMIZED_WARPS = 4\n",
        "    OPTIMIZED_STAGES = 2\n",
        "\n",
        "    output_tensor = torch.empty(output_shape, dtype=quant_state.dtype, device=device)\n",
        "\n",
        "    grid = lambda META: (triton.cdiv(total_elements_in_output, META['BLOCK_SIZE_ELEMENTS_PER_CHUNK']),)\n",
        "\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight_data, quant_state.absmax, quant_state.offset,\n",
        "        quant_state.state2.absmax, quant_state.state2.code, quant_state.code,\n",
        "        output_tensor,\n",
        "        TOTAL_ELEMENTS_IN_OUTPUT_TENSOR=total_elements_in_output,\n",
        "        BLOCK_SIZE_ELEMENTS_PER_CHUNK=OPTIMIZED_BR,\n",
        "        ELEMENTS_PER_GROUP_CONST=quant_state.blocksize,\n",
        "        LOG2_L2_BLOCK_SIZE_CONST_KERNEL=int(math.log2(quant_state.state2.blocksize)),\n",
        "        num_warps=OPTIMIZED_WARPS,\n",
        "        num_stages=OPTIMIZED_STAGES\n",
        "    )\n",
        "    return output_tensor\n",
        "\n",
        "def your_dequantize_nf4(weight_param):\n",
        "    return _your_dequantize_nf4(weight_param.weight.data, weight_param.weight.quant_state)\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "# 4. Compiled Forward Passes & Monkey-Patching\n",
        "# We define our custom compiled functions and then patch them into the\n",
        "# original library classes. This happens *before* the model is loaded.\n",
        "# ===========================================================================\n",
        "\n",
        "# ---- Patch 1: BitsAndBytes Linear4bit (The main fix for graph breaks) ----\n",
        "@torch.compile(fullgraph=True)\n",
        "def compiled_bnb_forward(self, x):\n",
        "    # Dequantize using the Triton kernel from Task A\n",
        "    dequantized_weight = your_dequantize_nf4(self)\n",
        "    # Perform standard linear operation\n",
        "    output = F.linear(x, dequantized_weight, self.bias)\n",
        "    return output\n",
        "\n",
        "# ---- Patch 2: LlamaMLP (as in the original notebook) ----\n",
        "@torch.compile(fullgraph=True)\n",
        "def compiled_llama_mlp_forward(self, x):\n",
        "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "# ---- Patch 3: LlamaRMSNorm (Fixing reviewer feedback) ----\n",
        "# Using rsqrt for better numerics and ensuring it's patched.\n",
        "@torch.compile(fullgraph=True)\n",
        "def compiled_rmsnorm_forward(self, hidden_states):\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "    # Using rsqrt as suggested\n",
        "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "    return (self.weight * hidden_states).to(input_dtype)\n",
        "\n",
        "# ---- Patch 4: LlamaForCausalLM (Fixing reviewer feedback) ----\n",
        "# This patches the entire model's forward pass to correctly upcast logits.\n",
        "@torch.compile(fullgraph=False, dynamic=True)\n",
        "def compiled_causal_lm_forward(\n",
        "    self,\n",
        "    input_ids=None,\n",
        "    attention_mask=None,\n",
        "    position_ids=None,\n",
        "    past_key_values=None,\n",
        "    inputs_embeds=None,\n",
        "    labels=None,\n",
        "    use_cache=None,\n",
        "    output_attentions=None,\n",
        "    output_hidden_states=None,\n",
        "    return_dict=None,\n",
        "):\n",
        "    # This is a simplified wrapper around the original forward pass\n",
        "    # It ensures full compilation of the core logic while allowing flexibility.\n",
        "\n",
        "    # We call the original forward method of the *base* LlamaForCausalLM class\n",
        "    # to avoid infinite recursion after patching.\n",
        "    outputs = LlamaForCausalLM.forward(\n",
        "        self,\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        labels=None, # Pass labels as None to prevent internal loss calculation\n",
        "        use_cache=use_cache,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "\n",
        "    logits = outputs.logits\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        # **Reviewer Fix:** Upcast logits to float32 before loss calculation\n",
        "        logits_for_loss = logits.float()\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits_for_loss.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "    # Manually reconstruct the output object\n",
        "    if not return_dict:\n",
        "        output = (logits,) + outputs[1:]\n",
        "        return (loss,) + output if loss is not None else output\n",
        "\n",
        "    from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "    return CausalLMOutputWithPast(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        past_key_values=outputs.past_key_values,\n",
        "        hidden_states=outputs.hidden_states,\n",
        "        attentions=outputs.attentions,\n",
        "    )\n",
        "\n",
        "print(\"--- Applying patches to library classes ---\")\n",
        "bnb_nn.Linear4bit.forward = compiled_bnb_forward\n",
        "LlamaMLP.forward = compiled_llama_mlp_forward\n",
        "LlamaRMSNorm.forward = compiled_rmsnorm_forward\n",
        "# Apply the main model forward patch last\n",
        "LlamaForCausalLM.forward = compiled_causal_lm_forward\n",
        "print(\"--- Patches applied successfully ---\")\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "# 5. Model & Tokenizer Loading\n",
        "# ===========================================================================\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 1024\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Use Unsloth's FastLanguageModel for optimized loading and setup\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# ===========================================================================\n",
        "# 6. PEFT & QLoRA Configuration\n",
        "# ===========================================================================\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = False, # GC must be disabled for fullgraph compile\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "# ===========================================================================\n",
        "# 7. Dataset and Trainer Setup\n",
        "# ===========================================================================\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:2%]\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    # *** CORRECTED ARGUMENT NAME ***\n",
        "    processing_class = tokenizer,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 15,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ===========================================================================\n",
        "# 8. Training and Analysis\n",
        "# ===========================================================================\n",
        "print(\"\\n--- Starting Training with Compiled & Patched Model ---\")\n",
        "start_time = time.time()\n",
        "trainer_stats = trainer.train()\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"\\n--- Training Finished ---\")\n",
        "print(f\"Total training time: {training_time:.2f} seconds\")\n",
        "\n",
        "# *** CORRECTED FINAL LOSS PRINTING ***\n",
        "if trainer.state.log_history:\n",
        "    final_log = trainer.state.log_history[-1]\n",
        "    final_loss = final_log.get('loss', final_log.get('train_loss', 'N/A'))\n",
        "    if isinstance(final_loss, float):\n",
        "        print(f\"Final training loss: {final_loss:.4f}\")\n",
        "\n",
        "# Print dynamo stats\n",
        "torch._dynamo.utils.compile_times()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "JiAnAwhH61sa",
        "outputId": "fb3c7721-1488-4a45-8c1b-048690abac6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Applying patches to library classes ---\n",
            "--- Patches applied successfully ---\n",
            "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "--- Starting Training with Compiled & Patched Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 4,206 | Num Epochs = 1 | Total steps = 15\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 22,544,384/1,000,000,000 (2.25% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.325600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.684400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.231400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.103800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.148700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.130300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.171200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.114800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.104000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.113900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.090300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Finished ---\n",
            "Total training time: 15.70 seconds\n",
            "Final training loss: 0.3908\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TorchDynamo compilation metrics:\\nFunction                              Runtimes (s)\\n------------------------------------  ------------------------------------------------------------------------------------------------------------------------------\\n_compile.compile_inner                0.2370, 0.0813, 2.4950, 0.0208, 0.6170, 0.5355, 0.3118, 0.1966, 0.3928, 0.4289, 0.0081, 0.1536\\nOutputGraph.call_user_compiler        2.3667, 0.3787, 0.4043, 0.1752, 0.1085, 0.2440, 0.3266, 0.0939\\n_recursive_pre_grad_passes            0.0046, 0.0011, 0.0028, 0.0007, 0.0018, 0.0008, 0.0008, 0.0009\\ncreate_aot_dispatcher_function        2.3577, 0.3707, 0.3955, 0.1715, 0.1023, 0.2367, 0.3203, 0.0897\\n_recursive_joint_graph_passes         0.1581, 0.0121, 0.0254, 0.0085, 0.0016, 0.0120, 0.0254, 0.0017\\ncompile_fx.<locals>.fw_compiler_base  1.8923, 0.1321, 0.0915, 0.0276, 0.0251, 0.0608, 0.0454, 0.0241\\ncompile_fx_inner                      1.8913, 0.0697, 0.1311, 0.0761, 0.0906, 0.0820, 0.0267, 0.0265, 0.0242, 0.0269, 0.0599, 0.0256, 0.0444, 0.0381, 0.0232, 0.0211\\ninductor_codecache_torch_key          0.2155\\nTritonBundler.read_and_emit           0.0003, 0.0002, 0.0006, 0.0002, 0.0004, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0005, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002\\nPyCodeCache.load_by_key_path          0.0904, 0.0596, 0.0972, 0.0618, 0.0651, 0.0617, 0.0129, 0.0143, 0.0127, 0.0145, 0.0329, 0.0119, 0.0239, 0.0183, 0.0118, 0.0119\\nasync_compile.wait                    0.0796, 0.0555, 0.0787, 0.0571, 0.0521, 0.0508, 0.0086, 0.0094, 0.0083, 0.0098, 0.0111, 0.0078, 0.0098, 0.0072, 0.0077, 0.0075\\ncompile_fx.<locals>.bw_compiler       0.0707, 0.0772, 0.0829, 0.0273, 0.0278, 0.0264, 0.0392, 0.0219\\npad_mm_benchmark                      0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ij505Mz4YDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}