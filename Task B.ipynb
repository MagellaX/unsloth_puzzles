{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate\n!pip install -q transformers\n!pip install -q peft\n!pip install -q bitsandbytes\n!pip install -q trl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:02:37.213955Z","iopub.execute_input":"2025-04-08T13:02:37.214273Z","iopub.status.idle":"2025-04-08T13:02:58.042122Z","shell.execute_reply.started":"2025-04-08T13:02:37.214247Z","shell.execute_reply":"2025-04-08T13:02:58.041191Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers, accelerate, bitsandbytes, trl\nprint(\"transformers version:\", transformers.__version__)\nprint(\"accelerate version:\", accelerate.__version__)\nprint(\"bitsandbytes version:\", bitsandbytes.__version__)\nprint(\"trl version:\", trl.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:02:58.043504Z","iopub.execute_input":"2025-04-08T13:02:58.043833Z","iopub.status.idle":"2025-04-08T13:03:03.369917Z","shell.execute_reply.started":"2025-04-08T13:02:58.043800Z","shell.execute_reply":"2025-04-08T13:03:03.369141Z"}},"outputs":[{"name":"stdout","text":"transformers version: 4.47.0\naccelerate version: 1.2.1\nbitsandbytes version: 0.45.5\ntrl version: 0.16.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n!pip install --no-deps cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n!pip install --no-deps unsloth\n!pip install transformers tf-keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:03:03.371237Z","iopub.execute_input":"2025-04-08T13:03:03.371566Z","iopub.status.idle":"2025-04-08T13:03:27.146858Z","shell.execute_reply.started":"2025-04-08T13:03:03.371546Z","shell.execute_reply":"2025-04-08T13:03:27.145786Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting xformers==0.0.29\n  Downloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.16.1)\nCollecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, xformers\nSuccessfully installed triton-3.2.0 xformers-0.0.29\nCollecting cut_cross_entropy\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting unsloth_zoo\n  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\nSuccessfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.3.17\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (0.1.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nCollecting unsloth\n  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth\nSuccessfully installed unsloth-2025.3.19\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: tensorflow<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.17.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.37.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.13.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.1.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Install PyTorch, torchvision, torchao nightlies\n!pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu126 # full options are cpu/cu118/cu121/cu124/cu126\n!pip install --pre torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu --no-cache-dir ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:03:27.148365Z","iopub.execute_input":"2025-04-08T13:03:27.148628Z","iopub.status.idle":"2025-04-08T13:05:57.974091Z","shell.execute_reply.started":"2025-04-08T13:03:27.148606Z","shell.execute_reply":"2025-04-08T13:05:57.973217Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/nightly/cu126\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.8.0.dev20250408%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torchvision-0.22.0.dev20250407%2Bcu126-cp310-cp310-linux_x86_64.whl.metadata (6.2 kB)\nCollecting torchao\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torchao-0.11.0.dev20250404%2Bcu126-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm0:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (89 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting pytorch-triton==3.3.0+git96316ce5 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.3.0%2Bgit96316ce5-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (75.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.8.0.dev20250407%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.3.0%2Bgit96316ce5-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (153.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/153.4 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/torchvision-0.22.0.dev20250407%2Bcu126-cp310-cp310-linux_x86_64.whl (7.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/torch-2.8.0.dev20250407%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl (858.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.9/858.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/torchao-0.11.0.dev20250404%2Bcu126-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchao, nvidia-cusparselt-cu12, sympy, pytorch-triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cufile-cu12, nvidia-cudnn-cu12, nvidia-cuda-nvrtc-cu12, torch, torchvision\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.3.19 requires tyro, which is not installed.\nunsloth-zoo 2025.3.17 requires tyro, which is not installed.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.8.0.dev20250407+cu126 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.8.0.dev20250407+cu126 which is incompatible.\nunsloth 2025.3.19 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.47.0 which is incompatible.\nunsloth 2025.3.19 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\nunsloth-zoo 2025.3.17 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.47.0 which is incompatible.\nunsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\nxformers 0.0.29 requires torch==2.5.1, but you have torch 2.8.0.dev20250407+cu126 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufile-cu12-1.11.1.6 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvtx-cu12-12.6.77 pytorch-triton-3.3.0+git96316ce5 sympy-1.13.3 torch-2.8.0.dev20250407+cu126 torchao-0.11.0.dev20250404+cu126 torchvision-0.22.0.dev20250407+cu126\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\nRequirement already satisfied: torchtune in /usr/local/lib/python3.10/dist-packages (0.5.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from torchtune) (3.3.1)\nRequirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.29.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.4.5)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.3.9)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.2.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.9.0)\nRequirement already satisfied: blobfile>=2 in /usr/local/lib/python3.10/dist-packages (from torchtune) (3.0.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtune) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtune) (4.67.1)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from torchtune) (2.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtune) (5.9.5)\nRequirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from torchtune) (11.0.0)\nRequirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (3.21.0)\nRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (2.3.0)\nRequirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (5.3.0)\nRequirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (3.11.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtune) (2.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\nRequirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->torchtune) (4.9.3)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->torchtune) (2024.11.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtune) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtune) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch.distributed.fsdp import fully_shard\n\nprint(\"Torch version:\", torch.__version__) \nprint(\"Successfully imported fully_shard!\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:08:09.775732Z","iopub.execute_input":"2025-04-08T13:08:09.775949Z","iopub.status.idle":"2025-04-08T13:08:09.780975Z","shell.execute_reply.started":"2025-04-08T13:08:09.775927Z","shell.execute_reply":"2025-04-08T13:08:09.780146Z"}},"outputs":[{"name":"stdout","text":"Torch version: 2.8.0.dev20250407+cu126\nSuccessfully imported fully_shard!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# %%writefile accelerate_fsdp_config.yaml\n\n# compute_environment: LOCAL_MACHINE\n# debug: false\n# distributed_type: FSDP\n# downcast_bf16: false\n# machine_rank: 0\n# main_training_function: main\n# mixed_precision: fp16\n# num_machines: 1\n# num_processes: 2\n# rdzv_backend: static\n# same_network: true\n# tpu_env: []\n# tpu_use_cluster: false\n# tpu_use_sudo: false\n# use_cpu: false\n\n# fsdp_config:\n#   fsdp_version: 2\n#   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n#   fsdp_backward_prefetch_policy: BACKWARD_PRE\n#   fsdp_cpu_ram_efficient_loading: true\n#   fsdp_forward_prefetch: false\n#   fsdp_offload_params: true  # Set to true if you need CPU offloading\n#   fsdp_sharding_strategy: FULL_SHARD\n#   fsdp_state_dict_type: FULL_STATE_DICT\n#   fsdp_sync_module_states: true\n#   fsdp_use_orig_params: true  \n#   fsdp_transformer_layer_cls_to_wrap: \"LlamaDecoderLayer\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:05:58.030399Z","iopub.status.idle":"2025-04-08T13:05:58.030727Z","shell.execute_reply":"2025-04-08T13:05:58.030593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install --pre --upgrade \\\n#    torch torchvision torchaudio \\\n#    --extra-index-url https://download.pytorch.org/whl/nightly/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:05:58.031401Z","iopub.status.idle":"2025-04-08T13:05:58.031631Z","shell.execute_reply":"2025-04-08T13:05:58.031540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy==1.23.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:08:09.781791Z","iopub.execute_input":"2025-04-08T13:08:09.782022Z","iopub.status.idle":"2025-04-08T13:08:17.125705Z","shell.execute_reply.started":"2025-04-08T13:08:09.781993Z","shell.execute_reply":"2025-04-08T13:08:17.124604Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.3.19 requires tyro, which is not installed.\nunsloth-zoo 2025.3.17 requires tyro, which is not installed.\nalbucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nalbumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nchex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\njax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\npymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nunsloth 2025.3.19 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.47.0 which is incompatible.\nunsloth 2025.3.19 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\nunsloth-zoo 2025.3.17 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.47.0 which is incompatible.\nunsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nxarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nxformers 0.0.29 requires torch==2.5.1, but you have torch 2.8.0.dev20250407+cu126 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.distributed.fsdp as fsdp\nimport inspect\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"FSDP module location:\", inspect.getfile(fsdp))\nprint(\"\\nAttributes in torch.distributed.fsdp:\")\nfor name, member in inspect.getmembers(fsdp):\n    print(f\"{name}: {type(member)}\")\n\n# Check if fully_shard exists and print its docstring\nif hasattr(fsdp, \"fully_shard\"):\n    print(\"\\nFunction 'fully_shard' is available.\")\n    print(\"Docstring for fully_shard:\\n\", fsdp.fully_shard.__doc__) \nelse:\n    print(\"\\nFunction 'fully_shard' is NOT available in this build.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:08:17.144486Z","iopub.execute_input":"2025-04-08T13:08:17.144683Z","iopub.status.idle":"2025-04-08T13:08:17.170574Z","shell.execute_reply.started":"2025-04-08T13:08:17.144665Z","shell.execute_reply":"2025-04-08T13:08:17.169820Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.8.0.dev20250407+cu126\nFSDP module location: /usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/__init__.py\n\nAttributes in torch.distributed.fsdp:\nBackwardPrefetch: <class 'enum.EnumMeta'>\nCPUOffload: <class 'type'>\nCPUOffloadPolicy: <class 'type'>\nFSDPModule: <class 'type'>\nFlatParameter: <class 'torch.distributed.fsdp._flat_param._FlatParameterMeta'>\nFullOptimStateDictConfig: <class 'type'>\nFullStateDictConfig: <class 'type'>\nFullyShardedDataParallel: <class 'type'>\nLocalOptimStateDictConfig: <class 'type'>\nLocalStateDictConfig: <class 'type'>\nMixedPrecision: <class 'type'>\nMixedPrecisionPolicy: <class 'type'>\nOffloadPolicy: <class 'type'>\nOptimStateDictConfig: <class 'type'>\nOptimStateKeyType: <class 'enum.EnumMeta'>\nShardedOptimStateDictConfig: <class 'type'>\nShardedStateDictConfig: <class 'type'>\nShardingStrategy: <class 'enum.EnumMeta'>\nStateDictConfig: <class 'type'>\nStateDictSettings: <class 'type'>\nStateDictType: <class 'enum.EnumMeta'>\nUnshardHandle: <class 'type'>\n__all__: <class 'list'>\n__builtins__: <class 'dict'>\n__cached__: <class 'str'>\n__doc__: <class 'NoneType'>\n__file__: <class 'str'>\n__loader__: <class '_frozen_importlib_external.SourceFileLoader'>\n__name__: <class 'str'>\n__package__: <class 'str'>\n__path__: <class 'list'>\n__spec__: <class '_frozen_importlib.ModuleSpec'>\n_common_utils: <class 'module'>\n_debug_utils: <class 'module'>\n_dynamo_utils: <class 'module'>\n_exec_order_utils: <class 'module'>\n_flat_param: <class 'module'>\n_fsdp_extensions: <class 'module'>\n_fully_shard: <class 'module'>\n_init_utils: <class 'module'>\n_limiter_utils: <class 'module'>\n_optim_utils: <class 'module'>\n_runtime_utils: <class 'module'>\n_shard_utils: <class 'module'>\n_state_dict_utils: <class 'module'>\n_traversal_utils: <class 'module'>\n_unshard_param_utils: <class 'module'>\n_wrap_utils: <class 'module'>\napi: <class 'module'>\nfully_shard: <class 'function'>\nfully_sharded_data_parallel: <class 'module'>\nregister_fsdp_forward_method: <class 'function'>\nwrap: <class 'module'>\n\nFunction 'fully_shard' is available.\nDocstring for fully_shard:\n \n    Apply fully sharded data parallelism (FSDP) to ``module``, where FSDP\n    shards module parameters, gradients, and optimizer states across data\n    parallel workers to save memory at the cost of communication.\n\n    At initialization, FSDP shards the module's parameters across the data\n    parallel workers given by ``mesh``. Before forward, FSDP all-gathers the\n    sharded parameters across the data-parallel workers to get the unsharded\n    parameters for forward computation. If ``reshard_after_forward`` is\n    ``True``, then FSDP frees the unsharded parameters after forward and\n    re-all-gathers them in backward before gradient computation. After gradient\n    computation, FSDP frees the unsharded parameters and reduce-scatters the\n    unsharded gradients across data-parallel workers.\n\n    This implementation represents the sharded parameters as :class:`DTensor` s\n    sharded on dim-0, while the unsharded parameters will be like the original\n    parameters on ``module`` (e.g. :class:`torch.Tensor` if originally\n    :class:`torch.Tensor`). A module\n    `forward pre-hook <https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook>`_\n    on ``module`` all-gathers the parameters, and a module\n    `forward hook <https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook>`_\n    on ``module`` frees them (if needed). Similar backward hooks all-gather\n    parameters and later free parameters and reduce-scatter gradients.\n\n    Since grouping multiple tensors together for one collective is critical for\n    communication efficiency, this implementation makes this grouping first\n    class. Calling :meth:`fully_shard` on ``module`` constructs one group that\n    includes the parameters in ``module.parameters()`` except those already\n    assigned to a group from an earlier call on a submodule. This means that\n    :meth:`fully_shard` should be called bottom-up on your model. Each group's\n    parameters are all-gathered in one collective, and its gradients are\n    reduce-scattered in one collective. Partitioning the model into multiple\n    groups (\"layer by layer\") allows for peak memory savings and communication/computation\n    overlap. Users generally should *not* call :meth:`fully_shard` only on the\n    topmost root module.\n\n    Args:\n        module (Union[nn.Module, List[nn.Module]): The module or modules to\n            shard with FSDP and group together for communication.\n        mesh (Optional[DeviceMesh]): This data parallel mesh defines the\n            sharding and device. If 1D, then parameters are fully sharded\n            across the 1D mesh (FSDP) with ``(Shard(0),)`` placement. If 2D,\n            then parameters are sharded across the 1st dim and replicated\n            across the 0th dim (HSDP) with ``(Replicate(), Shard(0))``\n            placement. The mesh's device type gives the device type used for\n            communication; if a CUDA or CUDA-like device type, then we use the\n            current device.\n        reshard_after_forward (Union[bool, int]): This controls the parameter\n            behavior after forward and can trade off memory and communication:\n\n            - If ``True``, then this reshards parameters after forward and\n              re-all-gathers in backward.\n            - If ``False``, then this keeps the unsharded parameters in memory\n              after forward and avoids the all-gather in backward.\n            - If an ``int``, then this represents the world size to reshard to\n              after forward. It should be a non-trivial divisor of the ``mesh``\n              shard dim size (i.e. excluding 1 and the dim size itself). A\n              choice may be the intra-node size (e.g. ``torch.cuda.device_count()``).\n              This allows the all-gather in backward to be over a smaller world\n              size at the cost of higher memory usage than setting to ``True``.\n            - The root FSDP state has its value specially set to ``False`` as a\n              heuristic since its parameters would typically be immediately\n              all-gathered for backward.\n            - After forward, the parameters registered to the module depend on\n              to this: The registered parameters are the sharded parameters if\n              ``True``; unsharded parameters if ``False``; and the paramters\n              resharded to the smaller mesh otherwise. To modify the parameters\n              between forward and backward, the registered parameters must be\n              the sharded parameters. For ``False`` or an ``int``, this can be\n              done by manually resharding via :meth:`reshard`.\n        shard_placement_fn (Optional[Callable[[nn.Parameter], Optional[Shard]]]):\n            This callable can be used to override the sharding placement for a\n            parameter to shard a parameter on a dimension other than dim-0. If\n            this callable returns a :class:`Shard` placement (not ``None``),\n            then FSDP will shard according to that placement (e.g. ``Shard(1)``).\n            If sharding on a nonzero dim, we currently require even sharding,\n            i.e. the tensor dim size on that dim must be divisible by the FSDP\n            shard mesh size.\n        mp_policy (MixedPrecisionPolicy): This controls the mixed precision\n            policy, which offers parameter/reduction mixed precision for this\n            module. See :class:`MixedPrecisionPolicy` for details.\n        offload_policy (OffloadPolicy): This controls the offloading policy,\n            which offers parameter/gradient/optimizer state offloading. See\n            :class:`OffloadPolicy` and its subclasses for details.\n        ignored_params: Optional(Set[nn.Parameter]): The set of parameters to be\n            ignored by FSDP. They will not be sharded, nor moved to the device\n            during init, nor have their gradients reduced in backward.\n\n    Returns:\n        FSDPModule: The module with FSDP applied (in-place).\n    \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile fsdp2-qlora-final.py\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    CPUOffload,\n    ShardingStrategy,\n    BackwardPrefetch,\n)\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport gc\nimport time\nfrom triton import jit\nimport triton\nimport triton.language as tl\n\n# Environment setup\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,\" \\\n    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\n# My custom Triton kernel for NF4 dequantization\n@triton.jit\ndef _your_dequantize_nf4_kernel(w_ptr, w_out, abs_ptrs,\n                               offset_ptr, \n                               abs2_ptrs, code2,\n                               block_size2, gsize, code, blocks: tl.constexpr, Br: tl.constexpr):\n    \"\"\"\n    Optimized kernel for dequantizing NF4 weights\n    \"\"\"\n    pid = tl.program_id(0)\n    \n    # Guard condition improves wave-front scheduling\n    if pid < gsize:\n        # Compute indices for absmax values with optimal coalescing\n        absmax_group = pid*blocks + tl.arange(0, blocks)\n        \n        # Load absmax values with cache control\n        absmax = tl.load(abs_ptrs + absmax_group, eviction_policy=\"evict_first\")\n\n        # Use inline assembly for efficient log2 calculation\n        lz = tl.inline_asm_elementwise(\n            asm=\"\"\"\n            {\n                clz.b32 $0, $1;\n            }\n            \"\"\",\n            constraints=(\"=r,r\"),\n            args=[block_size2],\n            dtype=(tl.int32),\n            is_pure=True,\n            pack=1,\n        )\n\n        # Calculate second-level absmax indices using CLZ result\n        absmax_group2 = (absmax_group)>>(31-lz)\n        \n        # Load scale factors with appropriate cache policies\n        real_absmax = tl.load(code2 + absmax, eviction_policy=\"evict_last\")\n        absmax2 = tl.load(abs2_ptrs + absmax_group2, eviction_policy=\"evict_last\")\n        offset = tl.load(offset_ptr, eviction_policy=\"evict_last\") \n        \n        # Calculate final scale factors with fused multiply-add\n        final_absmax = absmax2 * real_absmax + offset\n\n        # Calculate weight offsets using 2D memory pattern\n        w_off = pid*(Br//2) + tl.arange(0, blocks)[:, None]*(Br//(2*blocks)) + tl.arange(0, Br//(2*blocks))[None, :]\n\n        # Load packed weights\n        w_packed = tl.load(w_ptr + w_off, eviction_policy=\"evict_first\")\n        \n        # Interleave weights with themselves\n        w_packed2 = tl.interleave(w_packed, w_packed)\n        \n        # Calculate shift amounts for each position\n        shift_sh = tl.arange(0, blocks)[:, None]*(Br//(blocks)) + tl.arange(0, Br//(blocks))[None, :]\n        shift = tl.where(shift_sh % 2 == 0, 4, 0)\n\n        # Extract 4-bit values using calculated shifts\n        shifted_w = (w_packed2 >> shift) & 0xF\n        \n        # Load dequantized values from codebook\n        real_w = tl.load(shifted_w + code, eviction_policy=\"evict_last\")\n\n        # Apply scaling with broadcasting\n        scaled_w = (real_w * final_absmax[:, None])\n\n        # Calculate output offsets using tiled pattern\n        out_off = pid*Br + tl.arange(0, blocks)[:, None]*(Br//blocks) + tl.arange(0, Br//blocks)[None, :]\n        \n        # Store results\n        tl.store(w_out + out_off, scaled_w, eviction_policy=\"evict_first\")\n    return\n\ndef _your_dequantize_nf4(weight, quant_state):\n    \"\"\"\n    Internal implementation function\n    \"\"\"\n    # Get the current device\n    device = weight.device\n    \n    # Extract metadata from quantization state\n    out_dtype = quant_state.dtype\n    code = quant_state.code\n    absmax = quant_state.absmax\n    real_shape = quant_state.shape\n    block_size = quant_state.blocksize\n    absmax2 = quant_state.state2.absmax\n    code2 = quant_state.state2.code\n    block_size2 = quant_state.state2.blocksize\n    offset = quant_state.offset\n\n    # Calculate sizes\n    size = weight.shape[0]\n    out_size = size * 2\n\n    # Set processing block size\n    Br = 8192\n    blocks = Br // block_size\n    \n    # Calculate grid size\n    gsize = (triton.cdiv(out_size, Br))\n\n    # Optimize thread count based on GPU architecture\n    props = torch.cuda.get_device_properties(device)\n    \n    # Adjust max threads per SM based on architecture\n    if props.major == 8:\n        if props.minor == 9:  # Ada Lovelace\n            max_th = 24 * props.multi_processor_count\n        elif props.minor == 0:  # Ampere\n            max_th = 32 * props.multi_processor_count\n    elif props.major == 7:\n        max_th = 16 * props.multi_processor_count  # Turing\n    else:\n        # Default for other architectures\n        max_th = 16 * props.multi_processor_count\n    \n    # Wave-front scheduling optimization\n    resto = gsize % max_th\n    wave_sze = gsize if resto == 0 else gsize + (max_th - resto)\n\n    # Create output tensor with correct dtype handling\n    is_t4 = (props.major == 7 and props.minor == 5)\n    final_dtype = out_dtype if (out_dtype == torch.float16 or not is_t4) else torch.float16\n    \n    w_out = torch.empty(real_shape, device=device, \n                        dtype=final_dtype, \n                        requires_grad=False)\n    \n    # Launch kernel with wave-front scheduling\n    grid = lambda META: ((wave_sze,))\n    \n    # Launch with optimized parameters\n    _your_dequantize_nf4_kernel[grid](\n        weight, w_out,\n        absmax, offset,\n        absmax2, code2,\n        block_size2, gsize, code, blocks, Br,\n        num_warps=16,\n        num_stages=1,\n        maxnreg=8192,\n    )\n\n    # Return with correct dtype\n    return w_out\n\ndef your_dequantize_nf4(weight):\n    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)\n\ndef empty_cache():\n    \"\"\"Clean up memory\"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n\ndef initialize_distributed():\n    \"\"\"Initialize the distributed environment\"\"\"\n    rank = int(os.environ.get(\"RANK\", 0))\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    \n    if not dist.is_initialized():\n        dist.init_process_group(\"nccl\")\n    \n    torch.cuda.set_device(local_rank)\n    print(f\"[Rank {rank}] Initialized with local_rank {local_rank} out of {world_size} processes.\")\n    return rank, local_rank, world_size\n\n# Patch BitsAndBytes' Linear4bit for custom dequantization\ndef patch_bnb_for_custom_dequant():\n    from bitsandbytes.nn.modules import Linear4bit\n    \n    # Store original method\n    original_forward = Linear4bit.forward\n    \n    # Define new forward method using custom dequantization\n    def custom_dequant_forward(self, x):\n        # Use custom dequantization without torch.compile\n        weight = your_dequantize_nf4(self)\n        out = x @ weight.t()\n        return out\n    \n    # Apply patch\n    Linear4bit.forward = custom_dequant_forward\n    print(\"Patched Linear4bit with custom Triton dequantization kernel\")\n\nclass LoRAWrapper(nn.Module):\n    \"\"\"\n    Wrapper for FSDP with LoRA parameters\n    \"\"\"\n    def __init__(self, model, local_rank):\n        super().__init__()\n        self.model = model\n        \n        # Create parallel lists of parameters and their names\n        self.trainable_params = []\n        self.trainable_param_names = []\n        \n        # Find all trainable parameters (LoRA adapters)\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.trainable_param_names.append(name)\n                self.trainable_params.append(param)\n        \n        # Create a placeholder parameter list that we can wrap with FSDP\n        self.lora_params = nn.ParameterList(self.trainable_params)\n        \n        print(f\"Extracted {len(self.trainable_params)} trainable parameters for FSDP wrapping\")\n        \n        # Set up FSDP kwargs\n        fsdp_kwargs = {\n            \"sharding_strategy\": ShardingStrategy.FULL_SHARD,\n            \"device_id\": torch.device(f\"cuda:{local_rank}\"),\n            \"mixed_precision\": MixedPrecision(\n                param_dtype=torch.float16,\n                reduce_dtype=torch.float16,\n            ),\n            \"backward_prefetch\": BackwardPrefetch.BACKWARD_PRE,\n            \"cpu_offload\": CPUOffload(offload_params=True),\n            \"limit_all_gathers\": True,\n        }\n        \n        # Wrap the parameter list with FSDP\n        self.sharded_lora_params = FSDP(self.lora_params, **fsdp_kwargs)\n    \n    def forward(self, *args, **kwargs):\n        # Forward passes through to the original model\n        return self.model(*args, **kwargs)\n\ndef main():\n    # Initialize distributed\n    rank, local_rank, world_size = initialize_distributed()\n    \n    # Clean memory before we start\n    empty_cache()\n    \n    # Patch BitsAndBytes for custom dequantization\n    if rank == 0:\n        print(\"Patching BitsAndBytes with custom Triton kernel...\")\n    patch_bnb_for_custom_dequant()\n    \n    # Set default dtype\n    torch.set_default_dtype(torch.float16)\n    \n    start_time = time.time()\n    \n    # Using Llama 3.1 8B model\n    model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n    max_seq_length = 1024\n    print(f\"[Rank {rank}] Loading model {model_name}...\")\n    \n    # Quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map={\"\": local_rank},\n        attn_implementation=\"sdpa\"\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    # Apply gradient checkpointing BEFORE applying LoRA\n    model.gradient_checkpointing_enable()\n    if hasattr(model, \"enable_input_require_grads\"):\n        model.enable_input_require_grads()\n    \n    # Set up LoRA\n    lora_config = LoraConfig(\n        r=64,\n        lora_alpha=128,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_dropout=0,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n    )\n    \n    # Apply LoRA\n    model = get_peft_model(model, lora_config)\n    \n    # Make sure only LoRA parameters are trainable\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if \".lora_A.\" in name or \".lora_B.\" in name:\n                param.requires_grad_(True)\n            else:\n                param.requires_grad_(False)\n    \n    # Apply torch.compile strategically to avoid errors\n    if rank == 0:\n        print(\"Strategically applying torch.compile to non-LoRA layers...\")\n    \n    # Compile attention layers only for demonstration\n    for name, module in model.named_modules():\n        # Only compile specific non-LoRA parts\n        if (\"sdpa\" in name.lower() or \"rms_norm\" in name.lower()) and not any(x in name.lower() for x in [\"lora\", \"fsdp\"]):\n            try:\n                orig_forward = module.forward\n                # Use simple direct compilation without mode/options conflict\n                module.forward = torch.compile(orig_forward)\n                if rank == 0:\n                    print(f\"Applied torch.compile to {name}\")\n            except Exception as e:\n                if rank == 0:\n                    print(f\"Failed to compile {name}: {str(e)}\")\n    \n    # Wrap with FSDP specifically for LoRA parameters\n    model = LoRAWrapper(model, local_rank)\n    \n    # Print trainable parameters\n    if rank == 0:\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        all_params = sum(p.numel() for p in model.parameters())\n        print(f\"Trainable parameters: {trainable_params:,} ({100.0 * trainable_params / all_params:.2f}% of {all_params:,} total)\")\n    \n    # Load dataset\n    url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n    dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:5%]\")\n    \n    # Process dataset\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_seq_length,\n            return_tensors=\"pt\"\n        )\n    \n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        num_proc=2,\n        remove_columns=dataset.column_names\n    )\n    \n    # Create DataLoader\n    def collate_fn(examples):\n        batch = tokenizer.pad(\n            examples,\n            padding=\"longest\",\n            max_length=max_seq_length,\n            return_tensors=\"pt\"\n        )\n        batch[\"labels\"] = batch[\"input_ids\"].clone()\n        return batch\n    \n    from torch.utils.data import DataLoader\n    \n    train_dataloader = DataLoader(\n        tokenized_dataset, \n        batch_size=1,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    \n    # Log model initialization time\n    if rank == 0:\n        print(f\"Model initialization time: {time.time() - start_time:.2f} seconds\")\n        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        print(f\"CUDA memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n        print(\"Using FSDP2 with custom Triton kernel and torch.compile\")\n    \n    # Set up optimizer and loss function\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=2e-4,\n        weight_decay=0.01,\n        betas=(0.9, 0.95)\n    )\n    \n    # Training loop\n    num_epochs = 20\n    max_steps = 60\n    global_step = 0\n    \n    empty_cache()\n    \n    print(f\"[Rank {rank}] Starting training with FSDP2, QLoRA, and torch.compile\")\n    train_start = time.time()\n    \n    # Collect loss values\n    loss_values = []\n    \n    try:\n        for epoch in range(num_epochs):\n            model.train()\n            \n            for batch_idx, batch in enumerate(train_dataloader):\n                # Move batch to device\n                batch = {k: v.to(torch.device(f\"cuda:{local_rank}\")) for k, v in batch.items()}\n                \n                # Zero gradients\n                optimizer.zero_grad()\n                \n                # Forward pass\n                outputs = model(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    labels=batch[\"labels\"]\n                )\n                \n                loss = outputs.loss\n                \n                # Backward pass\n                loss.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                \n                # Update weights\n                optimizer.step()\n                \n                global_step += 1\n                \n                # Log progress\n                if rank == 0 and global_step % 5 == 0:\n                    with torch.no_grad():\n                        loss_item = loss.detach().item()\n                    loss_values.append(loss_item)\n                    print(f\"Epoch {epoch+1}/{num_epochs}, Step {global_step}, Loss: {loss_item:.4f}\")\n                \n                # Sync at each step\n                dist.barrier()\n                \n                # Check if we've reached max steps\n                if global_step >= max_steps:\n                    break\n            \n            if global_step >= max_steps:\n                break\n        \n        # Log training time\n        if rank == 0:\n            print(f\"Training completed in {time.time() - train_start:.2f} seconds\")\n            print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n            with torch.no_grad():\n                final_loss = loss.detach().item()\n            print(f\"Final Loss: {final_loss:.4f}\")\n            print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n            print(f\"CUDA memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n            \n            # Print loss values for comparison\n            print(f\"Loss values: {loss_values}\")\n            \n            # Save model - try using model.model to access the unwrapped model\n            try:\n                save_directory = \"./final_model\"\n                model.model.save_pretrained(save_directory)\n                tokenizer.save_pretrained(save_directory)\n                print(f\"Model saved to {save_directory}\")\n            except Exception as e:\n                print(f\"Error saving model: {e}\")\n    \n    except Exception as e:\n        print(f\"[Rank {rank}] Training failed with error: {e}\")\n        # Try to save even if we had an error\n        if rank == 0:\n            try:\n                model.model.save_pretrained(\"./emergency_save\")\n                print(\"Emergency save completed\")\n            except:\n                print(\"Could not complete emergency save\")\n    \n    # Clean up\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:59:10.500825Z","iopub.execute_input":"2025-04-08T13:59:10.501171Z","iopub.status.idle":"2025-04-08T13:59:10.509854Z","shell.execute_reply.started":"2025-04-08T13:59:10.501145Z","shell.execute_reply":"2025-04-08T13:59:10.508981Z"}},"outputs":[{"name":"stdout","text":"Overwriting fsdp2-qlora-final.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!accelerate launch --multi_gpu --num_processes 2 fsdp2-qlora-final.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:59:10.709279Z","iopub.execute_input":"2025-04-08T13:59:10.709591Z","iopub.status.idle":"2025-04-08T14:03:49.577171Z","shell.execute_reply.started":"2025-04-08T13:59:10.709564Z","shell.execute_reply":"2025-04-08T14:03:49.576045Z"}},"outputs":[{"name":"stdout","text":"2025-04-08 13:59:20.515110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-08 13:59:20.537680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-08 13:59:20.543816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-08 13:59:20.547793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-08 13:59:20.568229: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-08 13:59:20.574886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[Rank 0] Initialized with local_rank 0 out of 2 processes.\nPatching BitsAndBytes with custom Triton kernel...\nPatched Linear4bit with custom Triton dequantization kernel\n[Rank 0] Loading model unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit...\n[Rank 1] Initialized with local_rank 1 out of 2 processes.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\nPatched Linear4bit with custom Triton dequantization kernel\n[Rank 1] Loading model unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit...\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\nStrategically applying torch.compile to non-LoRA layers...\nExtracted 448 trainable parameters for FSDP wrapping\nExtracted 448 trainable parameters for FSDP wrapping\nTrainable parameters: 251,658,240 (5.25% of 4,792,258,560 total)\nModel initialization time: 10.51 seconds\nCUDA memory allocated: 5.95 GB\nCUDA memory reserved: 6.91 GB\nUsing FSDP2 with custom Triton kernel and torch.compile\n[Rank 0] Starting training with FSDP2, QLoRA, and torch.compile\n[Rank 1] Starting training with FSDP2, QLoRA, and torch.compile\nYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n[rank0]:[W408 13:59:39.597179584 ProcessGroupNCCL.cpp:4797] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n[rank1]:[W408 13:59:39.601167499 ProcessGroupNCCL.cpp:4797] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\nEpoch 1/20, Step 5, Loss: 0.1138\nEpoch 1/20, Step 10, Loss: 0.0833\nEpoch 1/20, Step 15, Loss: 0.1941\nEpoch 1/20, Step 20, Loss: 0.1028\nEpoch 1/20, Step 25, Loss: 0.1312\nEpoch 1/20, Step 30, Loss: 0.4133\nEpoch 1/20, Step 35, Loss: 0.1612\nEpoch 1/20, Step 40, Loss: 0.1141\nEpoch 1/20, Step 45, Loss: 0.3537\nEpoch 1/20, Step 50, Loss: 0.0769\nEpoch 1/20, Step 55, Loss: 0.1643\nEpoch 1/20, Step 60, Loss: 0.2476\nTraining completed in 249.49 seconds\nTotal time: 260.34 seconds\nFinal Loss: 0.2476\nCUDA memory allocated: 8.10 GB\nCUDA memory reserved: 10.44 GB\nLoss values: [0.11381540447473526, 0.08325958997011185, 0.19406253099441528, 0.10282765328884125, 0.1311759203672409, 0.41329607367515564, 0.1611652821302414, 0.11414648592472076, 0.35371196269989014, 0.07692121714353561, 0.16428713500499725, 0.24761340022087097]\nModel saved to ./final_model\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}